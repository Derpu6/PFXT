import streamlit as st
import os
import json
import time
import re
import ast
import pandas as pd
import numpy as np
import altair as alt
import hashlib
import difflib
from datetime import datetime
from collections import defaultdict
import dashscope
from dashscope import Generation
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# --- å…¨å±€é…ç½® ---
CONFIG_DIR = "exam_configs"
PLAGIARISM_DIR = "plagiarism_data"
RESULTS_DIR = "exam_results"
os.makedirs(CONFIG_DIR, exist_ok=True)
os.makedirs(PLAGIARISM_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)


# --- åˆå§‹åŒ–Session State ---
def init_session_state():
    """åˆå§‹åŒ–sessionçŠ¶æ€"""
    defaults = {
        'exam_config': None,
        'student_code': "",
        'scores': {},
        'comments': {},
        'api_key': "",
        'ai_feedback': {},
        'student_id': "",
        'student_name': "",
        'design_task': None,
        'language': "c",  # æ–°å¢ï¼šé»˜è®¤è¯­è¨€
        'app_state': {
            'is_admin': False,
            'show_stats': False,
            'switch_time': time.time(),
            'memory': None,
            'messages': [],
            'mcu_model': "51å•ç‰‡æœº"
        }
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

    if st.session_state['app_state']['memory'] is None:
        st.session_state['app_state']['memory'] = {
            'chat_history': [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯å•ç‰‡æœºåŠ©æ‰‹ï¼Œè¯·é—®ä½ æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"}]
        }


# --- Qwenæ¨¡å‹é›†æˆ ---
class QwenChat:
    def __init__(self, api_key):
        self.api_key = api_key

    def invoke(self, messages):
        """è°ƒç”¨Qwenæ¨¡å‹"""
        dashscope_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                dashscope_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                dashscope_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                dashscope_messages.append({"role": "assistant", "content": msg.content})

        try:
            response = Generation.call(
                model="qwen-plus",
                messages=dashscope_messages,
                temperature=0.3,
                api_key=self.api_key
            )

            if response.status_code != 200:
                raise Exception(f"API Error ({response.status_code}): {response.message}")

            return AIMessage(content=response.output.text)
        except Exception as e:
            raise Exception(f"è°ƒç”¨Qwenæ¨¡å‹å¤±è´¥: {str(e)}")


# --- å·¥å…·å‡½æ•° ---
def analyze_code(code_content, language="c"):
    """åˆ†æä»£ç è´¨é‡ - å¢å¼ºæ”¯æŒPython"""
    analysis = {
        "line_count": 0,
        "comment_count": 0,
        "comment_ratio": 0,
        "function_count": 0,
        "avg_function_length": 0,
        "issues": []
    }

    if not code_content:
        return analysis

    try:
        # åŸºæœ¬ç»Ÿè®¡
        lines = code_content.split('\n')
        analysis["line_count"] = len(lines)

        # è¯­è¨€ç‰¹å®šåˆ†æ
        if language == "python":
            # Pythonæ³¨é‡Šç»Ÿè®¡
            analysis["comment_count"] = sum(1 for line in lines if line.strip().startswith('#'))
            analysis["comment_ratio"] = analysis["comment_count"] / len(lines) * 100 if lines else 0

            try:
                tree = ast.parse(code_content)
                functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                analysis["function_count"] = len(functions)

                # è®¡ç®—å‡½æ•°é•¿åº¦
                func_lengths = []
                for func in functions:
                    func_lengths.append(func.end_lineno - func.lineno)

                if func_lengths:
                    analysis["avg_function_length"] = sum(func_lengths) / len(func_lengths)

                # Pythonç‰¹å®šé—®é¢˜æ£€æµ‹
                if "eval(" in code_content:
                    analysis["issues"].append("âš ï¸ å®‰å…¨é£é™©: ä½¿ç”¨eval()å‡½æ•°")
                if "exec(" in code_content:
                    analysis["issues"].append("âš ï¸ å®‰å…¨é£é™©: ä½¿ç”¨exec()å‡½æ•°")
                if any("import *" in line for line in lines):
                    analysis["issues"].append("âš ï¸ ä»£ç è§„èŒƒ: é¿å…ä½¿ç”¨é€šé…ç¬¦å¯¼å…¥")

            except Exception as e:
                analysis["error"] = f"Pythonè§£æé”™è¯¯: {str(e)}"
        else:
            # C/C++æ³¨é‡Šç»Ÿè®¡
            analysis["comment_count"] = code_content.count("//") + code_content.count("/*")
            analysis["comment_ratio"] = analysis["comment_count"] / len(lines) * 100 if lines else 0

            try:
                # C/C++ç»“æ„åˆ†æ
                function_pattern = r'\w+\s+\w+\([^)]*\)\s*{'
                functions = re.findall(function_pattern, code_content)
                analysis["function_count"] = len(functions)

                # å…¶ä»–C/C++ç‰¹å®šåˆ†æ...
                if "malloc" in code_content and code_content.count("malloc") > code_content.count("free"):
                    analysis["issues"].append("âš ï¸ èµ„æºæ³„æ¼é£é™©: å†…å­˜åˆ†é…ä¸é‡Šæ”¾ä¸åŒ¹é…")
            except Exception:
                pass

        # é€šç”¨é—®é¢˜æ£€æµ‹
        if analysis["comment_count"] == 0:
            analysis["issues"].append("âš ï¸ ç¼ºå°‘æ³¨é‡Š")

        if analysis["function_count"] < 3:
            analysis["issues"].append("âš ï¸ æ¨¡å—åŒ–ä¸è¶³: å‡½æ•°æ•°é‡è¿‡å°‘")

        if analysis.get("avg_function_length", 0) > 30:
            analysis["issues"].append("âš ï¸ å‡½æ•°è¿‡é•¿: å»ºè®®æ‹†åˆ†å‡½æ•°")

    except Exception as e:
        analysis["error"] = f"ä»£ç åˆ†æé”™è¯¯: {str(e)}"

    return analysis


def calculate_code_similarity(code1, code2):
    """è®¡ç®—ä¸¤ä¸ªä»£ç çš„ç›¸ä¼¼åº¦"""
    matcher = difflib.SequenceMatcher(None, code1, code2)
    return matcher.ratio() * 100


def calculate_hash(code):
    """è®¡ç®—ä»£ç å“ˆå¸Œå€¼ï¼ˆç”¨äºé¢„ç­›é€‰ï¼‰"""
    # æ ‡å‡†åŒ–ä»£ç ï¼šç§»é™¤ç©ºæ ¼ã€æ³¨é‡Šå’Œå˜é‡å
    normalized = re.sub(r'#.*?\n', '', code)  # ç§»é™¤Pythonå•è¡Œæ³¨é‡Š
    normalized = re.sub(r'//.*?\n', '', normalized)  # ç§»é™¤Cå•è¡Œæ³¨é‡Š
    normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)  # ç§»é™¤å¤šè¡Œæ³¨é‡Š
    normalized = re.sub(r'\'\'\'.*?\'\'\'', '', normalized, flags=re.DOTALL)  # ç§»é™¤Pythonå¤šè¡Œæ³¨é‡Š
    normalized = re.sub(r'""".*?"""', '', normalized, flags=re.DOTALL)  # ç§»é™¤Pythonå¤šè¡Œæ³¨é‡Š
    normalized = re.sub(r'\s+', '', normalized)  # ç§»é™¤æ‰€æœ‰ç©ºç™½
    normalized = re.sub(r'[a-zA-Z_][a-zA-Z0-9_]*', 'var', normalized)  # æ ‡å‡†åŒ–å˜é‡å
    return hashlib.md5(normalized.encode()).hexdigest()


def prefilter_codes(codes):
    """ä½¿ç”¨å“ˆå¸Œå€¼é¢„ç­›é€‰ç›¸ä¼¼ä»£ç """
    hash_map = defaultdict(list)
    for student, code in codes.items():
        code_hash = calculate_hash(code)
        hash_map[code_hash].append(student)

    return [group for group in hash_map.values() if len(group) > 1]


def analyze_plagiarism_for_exam(exam_name):
    """åˆ†ææŒ‡å®šè¯„åˆ†çš„æŠ„è¢­æƒ…å†µ - æ”¯æŒPythonæ–‡ä»¶"""
    plagiarism_dir = os.path.join(PLAGIARISM_DIR, exam_name)
    if not os.path.exists(plagiarism_dir):
        return None, "æ²¡æœ‰æ‰¾åˆ°è¯¥è¯„åˆ†çš„æäº¤è®°å½•"

    # è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶ï¼ˆæ”¯æŒ.cå’Œ.pyï¼‰
    code_files = [f for f in os.listdir(plagiarism_dir) if f.endswith(('.c', '.py'))]
    if len(code_files) < 2:
        return None, "æäº¤æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒæŠ„è¢­åˆ†æ"

    # è¯»å–æ‰€æœ‰ä»£ç 
    codes = {}
    for file in code_files:
        file_path = os.path.join(plagiarism_dir, file)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                codes[file] = f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    codes[file] = f.read()
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶{file_path}å¤±è´¥: {str(e)}")

    # å“ˆå¸Œé¢„ç­›é€‰
    hash_groups = prefilter_codes(codes)
    high_similarity_pairs = []

    for group in hash_groups:
        for i in range(len(group)):
            for j in range(i + 1, len(group)):
                student1 = group[i]
                student2 = group[j]
                similarity = calculate_code_similarity(codes[student1], codes[student2])

                if similarity > 80:
                    high_similarity_pairs.append({
                        "å­¦ç”Ÿ1": student1.replace('.c', '').replace('.py', ''),
                        "å­¦ç”Ÿ2": student2.replace('.c', '').replace('.py', ''),
                        "ç›¸ä¼¼åº¦": similarity
                    })

    return high_similarity_pairs, None


def generate_similarity_report(exam_name):
    """ç”ŸæˆæŠ„è¢­æƒ…å†µæŠ¥å‘Š"""
    high_similarity_pairs, error = analyze_plagiarism_for_exam(exam_name)

    if error:
        return None, error

    report = {
        "exam_name": exam_name,
        "high_similarity_pairs": high_similarity_pairs,
        "total_pairs": len(high_similarity_pairs)
    }

    return report, None


def ai_generate_exam_config(task_content, api_key):
    """ä½¿ç”¨AIç”Ÿæˆè¯„åˆ†é…ç½® - æ”¯æŒPythoné¡¹ç›®"""
    if not api_key:
        return None

    prompt = f"""
ä½ æ˜¯ä¸€ä½ç¼–ç¨‹è¯¾ç¨‹ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹é¡¹ç›®è®¾è®¡ä»»åŠ¡ä¹¦ç”Ÿæˆé€‚åˆåˆå­¦è€…çš„è¯„åˆ†é¢˜ç›®é…ç½®ã€‚
è¯·æ³¨æ„ï¼šå­¦ç”Ÿå¯èƒ½æäº¤å¤šç§è¯­è¨€ä»£ç ï¼ˆC/Pythonï¼‰ï¼Œè¯„åˆ†åº”å…³æ³¨æ ¸å¿ƒåŠŸèƒ½å®ç°å’Œä»£ç è´¨é‡ã€‚

**ä»»åŠ¡ä¹¦å†…å®¹**:
{task_content[:2500]}

**è¯·ç”ŸæˆåŒ…å«1-3é“é¢˜ç›®çš„è¯„åˆ†é…ç½®ï¼Œæ ¼å¼è¦æ±‚**:
{{
  "exam_name": "è¯„åˆ†åç§°",
  "exam_date": "YYYY-MM-DD",
  "questions": [
    {{
      "title": "é¢˜ç›®æ ‡é¢˜",
      "description": "é¢˜ç›®æè¿°",
      "total": åˆ†å€¼,
      "subtasks": [
        {{"desc": "åŠŸèƒ½ç‚¹æè¿°", "score": åˆ†å€¼}},
        ...
      ],
      "code_criteria": ["ä»£ç è´¨é‡è¦æ±‚1", "ä»£ç è´¨é‡è¦æ±‚2"]
    }},
    ...
  ]
}}

**æ³¨æ„äº‹é¡¹**:
1. é¢˜ç›®åº”å…³æ³¨æ ¸å¿ƒåŠŸèƒ½å®ç°ï¼Œè¯­è¨€æ— å…³
2. é¢˜ç›®æ€»åˆ†è®¾ç½®ä¸º85åˆ†ï¼ˆåŠ ä¸Š15åˆ†ä»£ç è´¨é‡åˆ†ï¼Œæ€»è®¡100åˆ†ï¼‰
3. è¯„åˆ†æ ‡å‡†è¦é€‚åˆåˆå­¦è€…ï¼Œéš¾åº¦é€‚ä¸­
4. åŠŸèƒ½ç‚¹åº”å…³æ³¨:
   - æ˜¯å¦æ­£ç¡®è¿›è¡Œåˆå§‹åŒ–/è®¾ç½®
   - ä¸»é€»è¾‘å®ç°
   - æ¨¡å—åŒ–è®¾è®¡ï¼ˆå‡½æ•°/ç±»åˆ’åˆ†ï¼‰
   - åŸºæœ¬é”™è¯¯å¤„ç†
5. ä»£ç è´¨é‡è¦æ±‚åº”å…³æ³¨:
   - ä»£ç ç»“æ„æ¸…æ™°
   - åŸºæœ¬æ³¨é‡Š
   - å‘½ååˆç†
   - ä»£ç ç®€æ´æ€§
6. é¿å…è¿‡äºä¸¥æ ¼çš„è¦æ±‚ï¼Œè€ƒè™‘åˆå­¦è€…æ°´å¹³

**ç¤ºä¾‹é…ç½®(Pythoné¡¹ç›®)**:
{{
  "exam_name": "æ•°æ®å¤„ç†é¡¹ç›®",
  "exam_date": "2023-11-15",
  "questions": [
    {{
      "title": "æ•°æ®åŠ è½½ä¸å¤„ç†",
      "description": "å®ç°æ•°æ®åŠ è½½å’Œå¤„ç†åŠŸèƒ½",
      "total": 50,
      "subtasks": [
        {{"desc": "æ­£ç¡®åŠ è½½æ•°æ®æ–‡ä»¶", "score": 15}},
        {{"desc": "å®ç°æ•°æ®æ¸…æ´—åŠŸèƒ½", "score": 20}},
        {{"desc": "æ•°æ®è½¬æ¢å¤„ç†", "score": 15}}
      ],
      "code_criteria": ["ä»£ç ç»“æ„æ¸…æ™°", "åŸºæœ¬æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½ååˆç†"]
    }},
    {{
      "title": "æ•°æ®åˆ†æåŠŸèƒ½",
      "description": "å®ç°åŸºæœ¬æ•°æ®åˆ†æåŠŸèƒ½",
      "total": 35,
      "subtasks": [
        {{"desc": "å®ç°ç»Ÿè®¡è®¡ç®—åŠŸèƒ½", "score": 15}},
        {{"desc": "æ•°æ®å¯è§†åŒ–è¾“å‡º", "score": 20}}
      ],
      "code_criteria": ["å‡½æ•°å°è£…åˆç†", "æ¨¡å—åŒ–è®¾è®¡"]
    }}
  ]
}}
"""

    try:
        qwen = QwenChat(api_key=api_key)
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„ç¼–ç¨‹æ•™å­¦ä¸“å®¶ï¼Œæ“…é•¿ä¸ºåˆå­¦è€…è®¾è®¡åˆç†çš„è¯„åˆ†é¢˜ç›®"),
            HumanMessage(content=prompt)
        ]
        response = qwen.invoke(messages)

        try:
            config = json.loads(response.content)
        except json.JSONDecodeError:
            match = re.search(r'\{.*\}', response.content, re.DOTALL)
            if match:
                config = json.loads(match.group(0))
            else:
                raise ValueError("æ— æ³•è§£æAIè¿”å›çš„JSON")

        return validate_and_adjust_config(config)
    except Exception as e:
        st.error(f"AIç”Ÿæˆé…ç½®å¤±è´¥: {str(e)}")
        return None


def validate_and_adjust_config(config):
    """éªŒè¯å¹¶è°ƒæ•´é…ç½®ä½¿å…¶é€‚åˆåˆå­¦è€…"""
    total_score = sum(q['total'] for q in config['questions'])
    if total_score != 85:
        scale = 85 / total_score
        for q in config['questions']:
            q['total'] = round(q['total'] * scale)

    if len(config['questions']) > 3:
        config['questions'] = config['questions'][:3]

    for q in config['questions']:
        if len(q['subtasks']) > 4:
            q['subtasks'] = q['subtasks'][:4]

        if len(q['code_criteria']) > 3:
            q['code_criteria'] = q['code_criteria'][:3]

        for subtask in q['subtasks']:
            if subtask['score'] > 20:
                subtask['score'] = 20

    return config


def ai_assistant_score(question, student_code, api_key, language="c"):
    """AIè¾…åŠ©è¯„åˆ† - æ”¯æŒPython"""
    if not api_key:
        return "é”™è¯¯: è¯·å…ˆè¾“å…¥APIå¯†é’¥"

    # æ ¹æ®è¯­è¨€æ·»åŠ ç‰¹å®šè¦æ±‚
    lang_specific = ""
    if language == "python":
        lang_specific = "\n**Pythonç‰¹å®šè¦æ±‚**:\n- ç¬¦åˆPEP8ä»£ç è§„èŒƒ\n- ä½¿ç”¨é€‚å½“çš„å¼‚å¸¸å¤„ç†\n- é¿å…ä½¿ç”¨eval()å’Œexec()\n- ä½¿ç”¨Pythonicçš„å†™æ³•"

    prompt = f"""
ä½ æ˜¯ä¸€ä½ç¼–ç¨‹è¯¾ç¨‹è¯„åˆ†ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹é¢˜ç›®è¦æ±‚è¯„ä¼°å­¦ç”Ÿä»£ç ï¼š

**é¢˜ç›®**: {question['title']}
**æè¿°**: {question['description']}
**åŠŸèƒ½ç‚¹è¦æ±‚**:{lang_specific}
"""
    for idx, subtask in enumerate(question['subtasks']):
        prompt += f"    {idx + 1}. {subtask['desc']} (åˆ†å€¼: {subtask['score']}åˆ†)\n"

    prompt += f"""
**ä»£ç è´¨é‡è¦æ±‚**: {', '.join(question['code_criteria'])}

**å­¦ç”Ÿä»£ç **: {student_code[:5000]} 

**è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»™å‡ºè¯„åˆ†å»ºè®®**ï¼š
1. **åŠŸèƒ½ç‚¹å®Œæˆæƒ…å†µ**ï¼ˆæ¯é¡¹åŠŸèƒ½ç‚¹å•ç‹¬è¯„ä¼°ï¼‰ï¼š
   - åŠŸèƒ½ç‚¹1: [å®ç°æƒ…å†µæè¿°] (å¾—åˆ†: x/y)
   - åŠŸèƒ½ç‚¹2: [å®ç°æƒ…å†µæè¿°] (å¾—åˆ†: x/y)
   ...
2. **ä»£ç è´¨é‡è¯„ä¼°**ï¼š
   - ä¼˜ç‚¹: [åˆ—å‡ºä»£ç çš„ä¼˜ç‚¹]
   - æ”¹è¿›å»ºè®®: [åˆ—å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹]
3. **æ€»ä½“è¯„ä»·ä¸å»ºè®®**:

ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚
"""

    try:
        qwen = QwenChat(api_key=api_key)
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ“…é•¿è¯„ä¼°å­¦ç”Ÿä»£ç è´¨é‡"),
            HumanMessage(content=prompt)
        ]
        response = qwen.invoke(messages)
        return response.content
    except Exception as e:
        return f"AIè¯„åˆ†å¤±è´¥: {str(e)}"


def create_exam_config_ui():
    """åˆ›å»ºè¯„åˆ†é…ç½®ç•Œé¢"""
    st.header("ğŸ“ åˆ›å»ºè¯„åˆ†é…ç½®")

    if 'exam_config' not in st.session_state or st.session_state.exam_config is None:
        st.session_state.exam_config = {
            'exam_name': '',
            'exam_date': '',
            'questions': [],
            'code_criteria': ["ä»£ç ç»“æ„æ¸…æ™°", "æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½ååˆç†"],
            'code_scores': [5, 5, 5]
        }

    config = st.session_state.exam_config
    questions = config.get('questions', [])

    st.subheader("1. ä¸Šä¼ é¡¹ç›®è®¾è®¡ä»»åŠ¡ä¹¦")
    uploaded_task = st.file_uploader("ä¸Šä¼ PDF/DOCXä»»åŠ¡ä¹¦", type=['pdf', 'docx'])

    if uploaded_task is not None:
        try:
            task_content = f"ä¸Šä¼ æ–‡ä»¶: {uploaded_task.name} (å†…å®¹æå–éœ€å®é™…å®ç°)"
            st.session_state.design_task = task_content
            st.success("ä»»åŠ¡ä¹¦å·²ä¸Šä¼ !")
        except Exception as e:
            st.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")

    if st.button("ğŸ¤– AIç”Ÿæˆè¯„åˆ†é…ç½®", disabled=not st.session_state.get('design_task', None)):
        with st.spinner("AIæ­£åœ¨ç”Ÿæˆè¯„åˆ†é…ç½®..."):
            config = ai_generate_exam_config(
                st.session_state.design_task,
                st.session_state.api_key
            )
            if config:
                st.session_state.exam_config = config
                st.success("è¯„åˆ†é…ç½®ç”ŸæˆæˆåŠŸ!")
                st.rerun()

    st.subheader("2. è°ƒæ•´è¯„åˆ†é…ç½®")
    if not st.session_state.get('exam_config', None):
        st.warning("è¯·å…ˆä¸Šä¼ ä»»åŠ¡ä¹¦å¹¶ç”Ÿæˆé…ç½®")
        return None

    config = st.session_state.exam_config
    questions = config.get('questions', [])

    col1, col2 = st.columns(2)
    with col1:
        exam_name = st.text_input("è¯„åˆ†åç§°", value=config.get('exam_name', ''))
    with col2:
        exam_date = st.text_input("è¯„åˆ†æ—¥æœŸ", value=config.get('exam_date', ''))

    st.session_state.exam_config['exam_name'] = exam_name
    st.session_state.exam_config['exam_date'] = exam_date

    st.subheader("å…¨å±€ä»£ç è´¨é‡è¦æ±‚")
    code_criteria = config.get('code_criteria', ["ä»£ç ç»“æ„æ¸…æ™°", "æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½ååˆç†"])
    code_scores = config.get('code_scores', [5, 5, 5])

    if len(code_scores) < len(code_criteria):
        code_scores.extend([5] * (len(code_criteria) - len(code_scores)))

    for j, criterion in enumerate(code_criteria):
        col1, col2 = st.columns([3, 1])
        with col1:
            crit_desc = st.text_input(f"è¦æ±‚ {j + 1}", value=criterion, key=f"crit_{j}_desc")
        with col2:
            crit_score = st.number_input("åˆ†å€¼", 1, 100,
                                         value=code_scores[j] if j < len(code_scores) else 5,
                                         key=f"crit_{j}_score")

        if j < len(code_criteria):
            code_criteria[j] = crit_desc
        if j < len(code_scores):
            code_scores[j] = crit_score

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ·»åŠ è¦æ±‚", key="add_crit"):
            code_criteria.append("æ–°è¦æ±‚")
            code_scores.append(5)
            st.session_state.exam_config['code_criteria'] = code_criteria
            st.session_state.exam_config['code_scores'] = code_scores
            st.rerun()
    with col2:
        if len(code_criteria) > 1 and st.button("â– åˆ é™¤è¦æ±‚", key="del_crit"):
            code_criteria.pop()
            code_scores.pop()
            st.session_state.exam_config['code_criteria'] = code_criteria
            st.session_state.exam_config['code_scores'] = code_scores
            st.rerun()

    st.session_state.exam_config['code_criteria'] = code_criteria
    st.session_state.exam_config['code_scores'] = code_scores

    st.subheader("é¢˜ç›®é…ç½®")

    for i, q in enumerate(questions):
        with st.expander(f"é¢˜ç›® {i + 1}: {q['title']}", expanded=True):
            col1, col2 = st.columns([3, 1])
            with col1:
                title = st.text_input("æ ‡é¢˜", value=q['title'], key=f"q{i}_title")
            with col2:
                total = st.number_input("æ€»åˆ†", 1, 100, value=q['total'], key=f"q{i}_total")

            description = st.text_area("æè¿°", value=q['description'],
                                       height=100, key=f"q{i}_desc")

            st.session_state.exam_config['questions'][i]['title'] = title
            st.session_state.exam_config['questions'][i]['total'] = total
            st.session_state.exam_config['questions'][i]['description'] = description

            st.markdown("**åŠŸèƒ½ç‚¹**")
            subtasks = q.get('subtasks', [])
            allocated_score = 0

            for j, subtask in enumerate(subtasks):
                current_score = st.session_state.exam_config['questions'][i]['subtasks'][j]['score']
                allocated_score += current_score

            for j, subtask in enumerate(subtasks):
                col1, col2 = st.columns([4, 1])
                with col1:
                    desc = st.text_input(f"åŠŸèƒ½ç‚¹ {j + 1} æè¿°", value=subtask['desc'], key=f"q{i}_sub{j}_desc")
                with col2:
                    max_score_val = q['total'] - allocated_score + subtask['score']
                    max_score = max(0, max_score_val)
                    initial_value = min(subtask['score'], max_score) if max_score > 0 else 0
                    score = st.number_input(
                        "åˆ†å€¼", 0, max_score, value=initial_value, key=f"q{i}_sub{j}_score"
                    )
                    allocated_score = allocated_score - subtask['score'] + score

                st.session_state.exam_config['questions'][i]['subtasks'][j]['desc'] = desc
                st.session_state.exam_config['questions'][i]['subtasks'][j]['score'] = score
                st.caption(f"å·²åˆ†é…: {allocated_score}/{q['total']} | å‰©ä½™: {q['total'] - allocated_score}")

            if allocated_score > q['total']:
                st.warning("âš ï¸ åŠŸèƒ½ç‚¹æ€»åˆ†å·²è¶…è¿‡é¢˜ç›®è®¾å®šï¼è¯·è°ƒæ•´åˆ†å€¼")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("â• æ·»åŠ åŠŸèƒ½ç‚¹", key=f"q{i}_add_sub"):
                    st.session_state.exam_config['questions'][i]['subtasks'].append({"desc": "æ–°åŠŸèƒ½ç‚¹", "score": 5})
                    st.rerun()
            with col2:
                if len(subtasks) > 1 and st.button("â– åˆ é™¤åŠŸèƒ½ç‚¹", key=f"q{i}_del_sub"):
                    st.session_state.exam_config['questions'][i]['subtasks'].pop()
                    st.rerun()

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ·»åŠ æ–°é¢˜ç›®"):
            st.session_state.exam_config['questions'].append({
                "title": f"é¢˜ç›® {len(questions) + 1}",
                "description": "",
                "total": 20,
                "subtasks": [{"desc": "ä¸»è¦åŠŸèƒ½", "score": 10}]
            })
            st.rerun()
    with col2:
        if len(questions) > 1 and st.button("â– åˆ é™¤é¢˜ç›®"):
            st.session_state.exam_config['questions'].pop()
            st.rerun()

    if st.button("ğŸ’¾ ä¿å­˜è¯„åˆ†é…ç½®"):
        config = {
            "exam_name": exam_name,
            "exam_date": exam_date,
            "questions": st.session_state.exam_config['questions'],
            "code_criteria": code_criteria,
            "code_scores": code_scores,
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M")
        }

        filename = f"{exam_name}_{exam_date}.json"
        filepath = os.path.join(CONFIG_DIR, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        st.session_state.exam_config = config
        st.success(f"é…ç½®å·²ä¿å­˜: {filepath}")
        return config

    return None


def load_exam_config_ui():
    """åŠ è½½è¯„åˆ†é…ç½®ç•Œé¢"""
    st.header("ğŸ“‚ åŠ è½½è¯„åˆ†é…ç½®")

    config_files = [f for f in os.listdir(CONFIG_DIR) if f.endswith(".json")]
    if not config_files:
        st.warning("æ²¡æœ‰æ‰¾åˆ°è¯„åˆ†é…ç½®æ–‡ä»¶")
        return None

    selected_file = st.selectbox("é€‰æ‹©è¯„åˆ†é…ç½®", config_files)
    filepath = os.path.join(CONFIG_DIR, selected_file)

    if st.button("åŠ è½½é…ç½®"):
        with open(filepath, "r", encoding='utf-8') as f:
            config = json.load(f)
            st.session_state.exam_config = config
            st.success(f"å·²åŠ è½½é…ç½®: {config['exam_name']}")
            return config
    return None


def scoring_interface(config):
    """è¯„åˆ†ç•Œé¢ - æ”¯æŒPythonæ–‡ä»¶"""
    if config is None:
        st.error("è¯„åˆ†é…ç½®æœªåŠ è½½ï¼")
        return

    st.header(f"ğŸ“ è¯„åˆ† - {config['exam_name']}")
    st.caption(f"è¯„åˆ†æ—¥æœŸ: {config['exam_date']}")

    st.subheader("å­¦ç”Ÿä¿¡æ¯")
    col1, col2 = st.columns(2)
    with col1:
        student_id = st.text_input("å­¦å·", value=st.session_state.get('student_id', ''))
    with col2:
        student_name = st.text_input("å§“å", value=st.session_state.get('student_name', ''))
    st.session_state.student_id = student_id
    st.session_state.student_name = student_name

    st.subheader("ä»£ç æäº¤")
    uploaded_file = st.file_uploader("ä¸Šä¼ å­¦ç”Ÿä»£ç ", type=['c', 'cpp', 'h', 'py'])
    code_content = st.session_state.get('student_code', "")

    # è¯­è¨€è¯†åˆ«
    if uploaded_file is not None:
        if uploaded_file.name.endswith('.py'):
            language = 'python'
        elif uploaded_file.name.endswith(('.cpp', '.h')):
            language = 'cpp'
        else:
            language = 'c'
        st.session_state.language = language  # å­˜å‚¨è¯­è¨€ç±»å‹

    if uploaded_file is not None:
        try:
            code_content = uploaded_file.getvalue().decode("utf-8")
        except UnicodeDecodeError:
            try:
                code_content = uploaded_file.getvalue().decode("gbk")
                st.warning("ä»£ç æ–‡ä»¶ä¼¼ä¹ä½¿ç”¨GBKç¼–ç ï¼Œå·²å°è¯•è½¬æ¢ã€‚è¯·ç¡®ä¿å†…å®¹æ­£ç¡®ã€‚")
            except UnicodeDecodeError:
                st.error("æ— æ³•è§£ç ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶æ˜¯æ–‡æœ¬æ ¼å¼ï¼ˆå¦‚ .c, .cpp, .h, .pyï¼‰å¹¶ä½¿ç”¨UTF-8æˆ–GBKç¼–ç ã€‚")
                code_content = ""

        if code_content:
            st.session_state.student_code = code_content
            with st.expander("æŸ¥çœ‹ä»£ç ", expanded=False):
                # æ ¹æ®è¯­è¨€æ˜¾ç¤ºä»£ç 
                language = st.session_state.language
                if language == "python":
                    st.code(code_content, language="python")
                else:
                    st.code(code_content, language="c")

    st.subheader("è¯„åˆ†")
    total_score = 0
    scores = {}
    comments = {}

    progress_bar = st.progress(0)
    num_questions = len(config.get('questions', []))

    if num_questions == 0:
        st.warning("è¯„åˆ†é…ç½®ä¸­æ²¡æœ‰é¢˜ç›®")
        return

    for i, q in enumerate(config['questions']):
        progress_value = (i + 1) / num_questions
        progress_bar.progress(progress_value)

        with st.expander(f"{q['title']} - {q['total']}åˆ†", expanded=(i == 0)):
            if code_content and st.button(f"ğŸ¤– AIè¾…åŠ©è¯„åˆ† - {q['title']}", key=f"ai_{i}", use_container_width=True):
                with st.spinner("AIè¯„åˆ†ä¸­..."):
                    feedback = ai_assistant_score(
                        q,
                        st.session_state.student_code,
                        st.session_state.api_key,
                        language=st.session_state.language
                    )
                    st.session_state.ai_feedback[q['title']] = feedback

            if q['title'] in st.session_state.get('ai_feedback', {}):
                st.subheader("ğŸ¤– AIè¯„åˆ†åé¦ˆ")
                st.info(st.session_state.ai_feedback[q['title']])
                st.divider()

            st.markdown(f"**åŠŸèƒ½å®ç° ({q['total']}åˆ†)**")
            func_score = 0
            q_comments = []

            for j, subtask in enumerate(q['subtasks']):
                col1, col2, col3 = st.columns([0.4, 0.3, 0.3])
                with col1:
                    st.markdown(f"**{subtask['desc']}**")
                with col2:
                    status = st.selectbox(
                        "å®Œæˆæƒ…å†µ",
                        ["æœªå®ç°", "éƒ¨åˆ†å®ç°", "å®Œå…¨å®ç°"],
                        index=["æœªå®ç°", "éƒ¨åˆ†å®ç°", "å®Œå…¨å®ç°"].index(
                            st.session_state.get(f"q{i}_sub{j}_status", "æœªå®ç°")),
                        key=f"q{i}_sub{j}_status"
                    )
                with col3:
                    max_score = float(subtask['score'])
                    if status == "æœªå®ç°":
                        score = 0.0
                    elif status == "éƒ¨åˆ†å®ç°":
                        default_partial = max_score / 2.0
                        score = st.number_input(
                            "å¾—åˆ†",
                            0.0, max_score, st.session_state.get(f"q{i}_sub{j}_score", default_partial),
                            key=f"q{i}_sub{j}_score",
                            step=0.5
                        )
                    else:
                        score = max_score
                    st.markdown(f"**å¾—åˆ†: {score:.1f}/{max_score}**")

                comment = st.text_area("è¯„è¯­", value=st.session_state.get(f"q{i}_sub{j}_comment", ""),
                                       key=f"q{i}_sub{j}_comment", height=60,
                                       placeholder="è®°å½•å®ç°ç»†èŠ‚ã€é—®é¢˜æˆ–å»ºè®®...")
                if comment:
                    q_comments.append(f"{subtask['desc']}: {comment}")

                func_score += score

            st.markdown(f"**é¢˜ç›®å¾—åˆ†: {func_score:.1f}/{q['total']}**")
            st.markdown("---")

            total_score += func_score
            scores[q['title']] = func_score
            comments[q['title']] = q_comments

    st.subheader("ä»£ç è´¨é‡è¯„åˆ†")
    code_criteria = config.get('code_criteria', [])
    code_scores = config.get('code_scores', [])
    code_total = sum(code_scores) if code_scores else 0

    if code_total > 0:
        st.markdown(f"**ä»£ç è´¨é‡ ({code_total}åˆ†)**")
        code_score = 0
        code_comments = []

        if code_criteria and code_scores:
            for j, (criterion, max_score) in enumerate(zip(code_criteria, code_scores)):
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.markdown(f"**{criterion}**")
                with col2:
                    score_val = st.number_input(
                        "å¾—åˆ†",
                        0.0, float(max_score),
                        st.session_state.get(f"code_crit{j}_score", max_score * 0.75),
                        key=f"code_crit{j}_score",
                        step=0.5
                    )
                    st.markdown(f"**å¾—åˆ†: {score_val:.1f}/{max_score}**")

                comment = st.text_area(f"{criterion}è¯„è¯­",
                                       value=st.session_state.get(f"code_crit{j}_comment", ""),
                                       key=f"code_crit{j}_comment", height=60,
                                       placeholder="è®°å½•ä»£ç è´¨é‡è¯„ä¼°...")
                if comment:
                    code_comments.append(f"{criterion}: {comment}")

                code_score += score_val
        else:
            st.warning("âš ï¸ æœªé…ç½®ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†")
            code_score = st.slider(
                "ä»£ç è´¨é‡è¯„åˆ†",
                0.0, 20.0, st.session_state.get("code_score", 15.0),
                key="code_score",
                step=0.5
            )

            code_comment = st.text_area("ä»£ç è´¨é‡è¯„è¯­", value=st.session_state.get("code_comment", ""),
                                        key="code_comment", height=80,
                                        placeholder="è®°å½•ä»£ç ç»“æ„ã€é£æ ¼ã€ä¼˜åŒ–å»ºè®®ç­‰...")
            if code_comment:
                code_comments.append(f"ä»£ç è´¨é‡: {code_comment}")

        if code_content:
            analysis = analyze_code(code_content, language=st.session_state.language)

            st.caption("ä»£ç åˆ†æç»“æœ:")
            col1, col2, col3 = st.columns(3)
            col1.metric("ä»£ç è¡Œæ•°", analysis["line_count"])
            col2.metric("æ³¨é‡Šæ•°é‡", analysis["comment_count"])
            col3.metric("æ³¨é‡Šæ¯”ä¾‹", f"{analysis['comment_ratio']:.1f}%")

            col1, col2 = st.columns(2)
            col1.metric("å‡½æ•°æ•°é‡", analysis["function_count"])
            col2.metric("å¹³å‡å‡½æ•°é•¿åº¦", f"{analysis['avg_function_length']:.1f}è¡Œ")

            if "issues" in analysis and analysis["issues"]:
                st.warning("æ½œåœ¨é—®é¢˜æ£€æµ‹")
                for issue in analysis["issues"]:
                    st.write(issue)

        st.markdown(f"**ä»£ç è´¨é‡å¾—åˆ†: {code_score:.1f}/{code_total}**")
        st.markdown("---")

        total_score += code_score
        scores["ä»£ç è´¨é‡"] = code_score
        comments["ä»£ç è´¨é‡"] = code_comments
    else:
        st.warning("âš ï¸ æœªé…ç½®ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†")

    st.session_state.scores = scores
    st.session_state.comments = comments

    st.subheader("æˆç»©æ¦‚è§ˆ")
    st.metric("æ€»åˆ†", f"{total_score:.1f}")

    st.subheader("æäº¤è¯„åˆ†")
    if st.button("âœ… æäº¤è¯„åˆ†", use_container_width=True, type="primary"):
        st.session_state.total_score = total_score

        if student_id and student_name:
            try:
                result_file = save_results(
                    student_id, student_name, config,
                    st.session_state.scores,
                    st.session_state.comments,
                    st.session_state.get('ai_feedback', {}),
                    code_content,
                    st.session_state.language
                )
                st.success(f"âœ… è¯„åˆ†ç»“æœå·²ä¿å­˜è‡³: {result_file}", icon="ğŸ‰")
            except Exception as e:
                st.error(f"âŒ ä¿å­˜è¯„åˆ†ç»“æœæ—¶å‡ºé”™: {e}")
        else:
            st.warning("âš ï¸ è¯·å¡«å†™å­¦å·å’Œå§“ååå†æäº¤ã€‚")


def save_results(student_id, student_name, config, scores, comments, ai_feedback, code_content, language="c"):
    """ä¿å­˜è¯„åˆ†ç»“æœ - æ”¯æŒPython"""
    result = {
        "student_id": student_id,
        "student_name": student_name,
        "exam_name": config['exam_name'],
        "exam_date": config['exam_date'],
        "score_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "total_score": sum(scores.values()),
        "scores": scores,
        "comments": comments,
        "ai_feedback": ai_feedback,
        "language": language
    }

    student_dir = f"{student_id}_{student_name}"
    os.makedirs(student_dir, exist_ok=True)

    student_result_file = os.path.join(student_dir, f"{config['exam_name']}_result.json")
    with open(student_result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    result_file = os.path.join(RESULTS_DIR, f"{student_id}_{student_name}_{config['exam_name']}_result.json")
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    if code_content:
        # æ ¹æ®è¯­è¨€ä¿å­˜ä¸åŒæ‰©å±•å
        ext = "py" if language == "python" else "c"
        code_file = os.path.join(student_dir, f"{config['exam_name']}_code.{ext}")
        with open(code_file, 'w', encoding='utf-8') as f:
            f.write(code_content)

        plagiarism_dir = os.path.join(PLAGIARISM_DIR, config['exam_name'])
        os.makedirs(plagiarism_dir, exist_ok=True)
        plagiarism_file = os.path.join(plagiarism_dir, f"{student_id}_{student_name}.{ext}")
        with open(plagiarism_file, 'w', encoding='utf-8') as f:
            f.write(code_content)

    return result_file


def show_learning_feedback():
    """æ˜¾ç¤ºå­¦æƒ…åé¦ˆç•Œé¢"""
    st.header("ğŸ“Š å­¦æƒ…åé¦ˆ")

    if st.session_state.exam_config is None:
        st.warning("è¯·å…ˆåŠ è½½æˆ–åˆ›å»ºä¸€ä¸ªè¯„åˆ†é…ç½®ï¼")
        return

    exam_name = st.session_state.exam_config['exam_name']
    st.subheader(f"å½“å‰è¯„åˆ†: {exam_name}")

    st.subheader("ç­çº§æ•´ä½“è¡¨ç°")

    if not os.path.exists(RESULTS_DIR):
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„åˆ†ç»“æœæ•°æ®")
        return

    result_files = [f for f in os.listdir(RESULTS_DIR) if f.endswith(".json")]
    if not result_files:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„åˆ†ç»“æœæ–‡ä»¶")
        return

    exam_results = []
    for file in result_files:
        if exam_name in file:
            filepath = os.path.join(RESULTS_DIR, file)
            with open(filepath, 'r', encoding='utf-8') as f:
                result = json.load(f)
                exam_results.append(result)

    if not exam_results:
        st.warning(f"æ²¡æœ‰æ‰¾åˆ°'{exam_name}'çš„è¯„åˆ†ç»“æœ")
        return

    students = []
    scores = []
    for result in exam_results:
        students.append(f"{result['student_id']}_{result['student_name']}")
        scores.append(result['total_score'])

    avg_score = np.mean(scores) if scores else 0
    max_score = max(scores) if scores else 0
    min_score = min(scores) if scores else 0

    col1, col2, col3 = st.columns(3)
    col1.metric("å¹³å‡åˆ†", f"{avg_score:.1f}")
    col2.metric("æœ€é«˜åˆ†", max_score)
    col3.metric("æœ€ä½åˆ†", min_score)

    st.write("å­¦ç”Ÿæˆç»©åˆ†å¸ƒ:")
    chart_data = pd.DataFrame({'å­¦ç”Ÿ': students, 'åˆ†æ•°': scores})
    chart = alt.Chart(chart_data).mark_bar().encode(
        x=alt.X('å­¦ç”Ÿ', sort=None),
        y='åˆ†æ•°',
        color=alt.value('skyblue')
    ).properties(width=600, height=300)
    text = chart.mark_text(align='center', baseline='bottom', dy=-5).encode(text='åˆ†æ•°')
    st.altair_chart(chart + text)

    st.subheader("æˆç»©åˆ†å¸ƒåˆ†æ")
    st.write(f"- ä¼˜ç§€ (â‰¥90åˆ†): {len([s for s in scores if s >= 90])}äºº")
    st.write(f"- è‰¯å¥½ (80-89åˆ†): {len([s for s in scores if 80 <= s < 90])}äºº")
    st.write(f"- ä¸­ç­‰ (70-79åˆ†): {len([s for s in scores if 70 <= s < 80])}äºº")
    st.write(f"- åŠæ ¼ (60-69åˆ†): {len([s for s in scores if 60 <= s < 70])}äºº")
    st.write(f"- ä¸åŠæ ¼ (<60åˆ†): {len([s for s in scores if s < 60])}äºº")

    st.subheader("ç­çº§å¼ºé¡¹ä¸å¼±é¡¹åˆ†æ")
    topic_scores = {}
    topic_counts = {}
    question_map = {
        q["title"]: q["total"]
        for q in st.session_state.exam_config.get("questions", [])
    }

    for result in exam_results:
        for topic, score in result["scores"].items():
            if topic == "ä»£ç è´¨é‡":
                total = st.session_state.exam_config.get("code_total", 15)
            else:
                total = question_map.get(topic, 100)
            score_rate = (score / total) * 100 if total > 0 else 0

            if topic not in topic_scores:
                topic_scores[topic] = 0
                topic_counts[topic] = 0

            topic_scores[topic] += score_rate
            topic_counts[topic] += 1

    topics = []
    avg_topic_scores = []
    for topic, total_score in topic_scores.items():
        count = topic_counts[topic]
        avg_score = total_score / count
        topics.append(topic)
        avg_topic_scores.append(avg_score)

    strong_topics = []
    weak_topics = []
    for i, score in enumerate(avg_topic_scores):
        if score >= 85:
            strong_topics.append(topics[i])
        elif score < 70:
            weak_topics.append(topics[i])

    col1, col2 = st.columns(2)
    with col1:
        st.success("**ç­çº§å¼ºé¡¹**")
        if strong_topics:
            for topic in strong_topics:
                st.write(f"- {topic} (å¾—åˆ†ç‡: {avg_topic_scores[topics.index(topic)]:.1f}%)")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼ºé¡¹")
    with col2:
        st.warning("**ç­çº§å¼±é¡¹**")
        if weak_topics:
            for topic in weak_topics:
                st.write(f"- {topic} (å¾—åˆ†ç‡: {avg_topic_scores[topics.index(topic)]:.1f}%)")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼±é¡¹")

    st.subheader("å„é¢˜ç›®å¾—åˆ†ç‡")
    topic_df = pd.DataFrame({'é¢˜ç›®': topics, 'å¹³å‡å¾—åˆ†ç‡': avg_topic_scores})
    chart = alt.Chart(topic_df).mark_bar().encode(
        x=alt.X('é¢˜ç›®', sort=None, axis=alt.Axis(labelAngle=45)),
        y=alt.Y('å¹³å‡å¾—åˆ†ç‡', scale=alt.Scale(domain=[0, 100])),
        color=alt.Color('å¹³å‡å¾—åˆ†ç‡:Q',
                        scale=alt.Scale(domain=[0, 70, 85, 100],
                                        range=['red', 'skyblue', 'green', 'green']),
                        legend=None)
    ).properties(width=600, height=400)
    rule_85 = alt.Chart(pd.DataFrame({'y': [85]})).mark_rule(color='green', strokeDash=[5, 5]).encode(y='y')
    rule_70 = alt.Chart(pd.DataFrame({'y': [70]})).mark_rule(color='red', strokeDash=[5, 5]).encode(y='y')
    text = chart.mark_text(align='center', baseline='bottom', dy=-5).encode(text=alt.Text('å¹³å‡å¾—åˆ†ç‡:Q', format='.1f'))
    st.altair_chart(chart + rule_85 + rule_70 + text)

    st.subheader("ä¸ªäººåˆ†æ•°åˆ†æ")
    selected_student = st.selectbox("é€‰æ‹©å­¦ç”Ÿ", students)
    student_result = None
    for result in exam_results:
        if f"{result['student_id']}_{result['student_name']}" == selected_student:
            student_result = result
            break

    if not student_result:
        st.warning("æ‰¾ä¸åˆ°è¯¥å­¦ç”Ÿçš„è¯¦ç»†ç»“æœ")
        return

    st.metric(f"{selected_student}çš„åˆ†æ•°", f"{student_result['total_score']}åˆ†")
    st.write("å…·ä½“è¡¨ç°:")
    col1, col2 = st.columns(2)
    with col1:
        st.info("**å¼ºé¡¹**")
        strong_topics = []
        for topic, score in student_result['scores'].items():
            for q in st.session_state.exam_config.get('questions', []):
                if q['title'] == topic:
                    total = q['total']
                    break
            else:
                total = 100
            score_rate = (score / total) * 100
            if score_rate >= 85:
                strong_topics.append(f"{topic} ({score_rate:.1f}%)")
        if strong_topics:
            for topic in strong_topics:
                st.write(f"- {topic}")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼ºé¡¹")
    with col2:
        st.warning("**å¼±é¡¹**")
        weak_topics = []
        for topic, score in student_result['scores'].items():
            for q in st.session_state.exam_config.get('questions', []):
                if q['title'] == topic:
                    total = q['total']
                    break
            else:
                total = 100
            score_rate = (score / total) * 100
            if score_rate < 70:
                weak_topics.append(f"{topic} ({score_rate:.1f}%)")
        if weak_topics:
            for topic in weak_topics:
                st.write(f"- {topic}")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼±é¡¹")


def show_plagiarism_report():
    """æ˜¾ç¤ºæŠ„è¢­æƒ…å†µæŠ¥å‘Š"""
    st.header("ğŸ” æŠ„è¢­æƒ…å†µåˆ†æ")

    exam_names = [d for d in os.listdir(PLAGIARISM_DIR) if os.path.isdir(os.path.join(PLAGIARISM_DIR, d))]
    if not exam_names:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä½œä¸šçš„æŠ„è¢­æ•°æ®")
        return

    selected_exam = st.selectbox("é€‰æ‹©ä½œä¸š", exam_names)

    if st.button("åˆ†ææŠ„è¢­æƒ…å†µ"):
        with st.spinner("æ­£åœ¨åˆ†ææŠ„è¢­æƒ…å†µ..."):
            report, error = generate_similarity_report(selected_exam)

        if error:
            st.warning(error)
            return

        st.subheader(f"ä½œä¸š: {selected_exam}")
        st.caption(f"é«˜ç›¸ä¼¼åº¦é…å¯¹æ•°é‡: {report['total_pairs']}")

        if report['high_similarity_pairs']:
            st.subheader("é«˜ç›¸ä¼¼åº¦é…å¯¹ (ç›¸ä¼¼åº¦ > 80%)")
            for pair in report['high_similarity_pairs']:
                st.warning(f"âš ï¸ {pair['å­¦ç”Ÿ1']} å’Œ {pair['å­¦ç”Ÿ2']} çš„ä»£ç ç›¸ä¼¼åº¦é«˜è¾¾ {pair['ç›¸ä¼¼åº¦']:.1f}%")

            st.subheader("é«˜ç›¸ä¼¼åº¦å­¦ç”Ÿå¯¹æ¯”")
            df = pd.DataFrame(report['high_similarity_pairs'])
            st.dataframe(df)

            st.subheader("ç›¸ä¼¼åº¦åˆ†å¸ƒ")
            similarities = [pair['ç›¸ä¼¼åº¦'] for pair in report['high_similarity_pairs']]
            sim_df = pd.DataFrame({'ç›¸ä¼¼åº¦': similarities})
            chart = alt.Chart(sim_df).mark_bar(color='salmon').encode(
                alt.X('ç›¸ä¼¼åº¦:Q', bin=alt.Bin(maxbins=10), title='ç›¸ä¼¼åº¦ (%)'),
                alt.Y('count()', title='é…å¯¹æ•°é‡'),
            ).properties(width=600, height=300)
            st.altair_chart(chart)
        else:
            st.success("âœ… æ²¡æœ‰å‘ç°é«˜ç›¸ä¼¼åº¦ä»£ç ")

    st.subheader("æŠ„è¢­æ£€æµ‹å»ºè®®")
    st.write("1. åŠ å¼ºä»£ç å®¡æŸ¥å’Œäººå·¥æ£€æŸ¥")
    st.write("2. ä½¿ç”¨æ›´å…ˆè¿›çš„æŠ„è¢­æ£€æµ‹å·¥å…·")
    st.write("3. å¯¹å­¦ç”Ÿè¿›è¡Œå­¦æœ¯è¯šä¿¡æ•™è‚²")
    st.write("4. è®¾è®¡æ›´å…·ä¸ªæ€§åŒ–çš„ç¼–ç¨‹é¢˜ç›®")
    st.write("5. å¢åŠ é¢è¯•ç¯èŠ‚éªŒè¯å­¦ç”Ÿç†è§£ç¨‹åº¦")


if __name__ == "__main__":
    init_session_state()
    st.sidebar.title("å¯¼èˆª")

    st.sidebar.subheader("AI APIå¯†é’¥è®¾ç½®")
    api_key = st.sidebar.text_input("è¾“å…¥AI APIå¯†é’¥", type="password",
                                    value=st.session_state.get('api_key', ''),
                                    help="ä»é˜¿é‡Œäº‘DashScopeå¹³å°è·å–")
    st.session_state.api_key = api_key

    app_mode = st.sidebar.selectbox("é€‰æ‹©æ¨¡å¼", ["è¯„åˆ†ç•Œé¢", "åˆ›å»ºè¯„åˆ†é…ç½®", "å­¦æƒ…åé¦ˆ", "æŠ„è¢­æƒ…å†µ"])

    st.sidebar.markdown("---")
    st.sidebar.subheader("åŠ è½½è¯„åˆ†é…ç½®")
    config_files = [f for f in os.listdir(CONFIG_DIR) if f.endswith(".json")]
    if config_files:
        selected_file = st.sidebar.selectbox("é€‰æ‹©è¯„åˆ†é…ç½®", config_files)
        filepath = os.path.join(CONFIG_DIR, selected_file)
        if st.sidebar.button("åŠ è½½é…ç½®"):
            with open(filepath, "r", encoding='utf-8') as f:
                config = json.load(f)
                st.session_state.exam_config = config
                st.sidebar.success(f"å·²åŠ è½½é…ç½®: {config['exam_name']}")
    else:
        st.sidebar.warning("æ²¡æœ‰æ‰¾åˆ°è¯„åˆ†é…ç½®æ–‡ä»¶")

    if app_mode == "åˆ›å»ºè¯„åˆ†é…ç½®":
        config = create_exam_config_ui()
        if config:
            st.session_state.exam_config = config
            st.success("è¯„åˆ†é…ç½®å·²åˆ›å»ºå¹¶åŠ è½½!")
    elif app_mode == "è¯„åˆ†ç•Œé¢":
        if st.session_state.exam_config:
            scoring_interface(st.session_state.exam_config)
        else:
            st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½è¯„åˆ†é…ç½®")
    elif app_mode == "å­¦æƒ…åé¦ˆ":
        if st.session_state.exam_config:
            show_learning_feedback()
        else:
            st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½è¯„åˆ†é…ç½®")
    elif app_mode == "æŠ„è¢­æƒ…å†µ":
        show_plagiarism_report()
