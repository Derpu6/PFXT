import streamlit as st
import os
import json
import time
import re
import ast
import pandas as pd
import numpy as np
import altair as alt
import hashlib
import difflib
from datetime import datetime
from collections import defaultdict
import dashscope
from dashscope import Generation
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# --- å…¨å±€é…ç½® ---
CONFIG_DIR = "exam_configs"
PLAGIARISM_DIR = "plagiarism_data"
RESULTS_DIR = "exam_results"  # æ·»åŠ ç»“æœç›®å½•
os.makedirs(CONFIG_DIR, exist_ok=True)
os.makedirs(PLAGIARISM_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)  # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨

# --- åˆå§‹åŒ–Session State ---
def init_session_state():
    """åˆå§‹åŒ–sessionçŠ¶æ€"""
    defaults = {
        'exam_config': None,
        'student_code': "",
        'scores': {},
        'comments': {},
        'api_key': "",
        'ai_feedback': {},
        'student_id': "",
        'student_name': "",
        'design_task': None,
        'app_state': {
            'is_admin': False,
            'show_stats': False,
            'switch_time': time.time(),
            'memory': None,
            'messages': [],
            'mcu_model': "51å•ç‰‡æœº"
        }
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

    # ç¡®ä¿memoryè¢«æ­£ç¡®åˆå§‹åŒ–
    if st.session_state['app_state']['memory'] is None:
        st.session_state['app_state']['memory'] = {
            'chat_history': [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯å•ç‰‡æœºåŠ©æ‰‹ï¼Œè¯·é—®ä½ æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"}]
        }


# --- Qwenæ¨¡å‹é›†æˆ ---
class QwenChat:
    def __init__(self, api_key):
        self.api_key = api_key

    def invoke(self, messages):
        """è°ƒç”¨Qwenæ¨¡å‹"""
        dashscope_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                dashscope_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                dashscope_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                dashscope_messages.append({"role": "assistant", "content": msg.content})

        try:
            response = Generation.call(
                model="qwen-plus",
                messages=dashscope_messages,
                temperature=0.3,
                api_key=self.api_key
            )

            if response.status_code != 200:
                raise Exception(f"API Error ({response.status_code}): {response.message}")

            return AIMessage(content=response.output.text)
        except Exception as e:
            raise Exception(f"è°ƒç”¨Qwenæ¨¡å‹å¤±è´¥: {str(e)}")


# --- å·¥å…·å‡½æ•° ---
def analyze_code(code_content):
    """åˆ†æä»£ç è´¨é‡"""
    analysis = {
        "line_count": 0,
        "comment_count": 0,
        "comment_ratio": 0,
        "function_count": 0,
        "avg_function_length": 0,
        "issues": []
    }

    if not code_content:
        return analysis

    try:
        # åŸºæœ¬ç»Ÿè®¡
        lines = code_content.split('\n')
        analysis["line_count"] = len(lines)

        comment_count = code_content.count("//") + code_content.count("/*")
        analysis["comment_count"] = comment_count
        analysis["comment_ratio"] = comment_count / len(lines) * 100 if lines else 0

        # ç»“æ„åˆ†æ
        try:
            tree = ast.parse(code_content)
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            analysis["function_count"] = len(functions)

            # å‡½æ•°é•¿åº¦åˆ†æ
            func_lengths = []
            for func in functions:
                func_lengths.append(func.end_lineno - func.lineno)

            if func_lengths:
                analysis["avg_function_length"] = sum(func_lengths) / len(func_lengths)
        except Exception:
            pass

        # æ½œåœ¨é—®é¢˜æ£€æµ‹
        if comment_count == 0:
            analysis["issues"].append("âš ï¸ ç¼ºå°‘æ³¨é‡Š")

        if "malloc" in code_content and code_content.count("malloc") > code_content.count("free"):
            analysis["issues"].append("âš ï¸ èµ„æºæ³„æ¼é£é™©: å†…å­˜åˆ†é…ä¸é‡Šæ”¾ä¸åŒ¹é…")

        if analysis["function_count"] < 3:
            analysis["issues"].append("âš ï¸ æ¨¡å—åŒ–ä¸è¶³: å‡½æ•°æ•°é‡è¿‡å°‘")

        if analysis["avg_function_length"] > 30:
            analysis["issues"].append("âš ï¸ å‡½æ•°è¿‡é•¿: å»ºè®®æ‹†åˆ†å‡½æ•°")

    except Exception as e:
        analysis["error"] = f"ä»£ç åˆ†æé”™è¯¯: {str(e)}"

    return analysis


def calculate_code_similarity(code1, code2):
    """è®¡ç®—ä¸¤ä¸ªä»£ç çš„ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
    matcher = difflib.SequenceMatcher(None, code1, code2)
    return matcher.ratio() * 100


def calculate_hash(code):
    """è®¡ç®—ä»£ç å“ˆå¸Œå€¼ï¼ˆç”¨äºé¢„ç­›é€‰ï¼‰"""
    # æ ‡å‡†åŒ–ä»£ç ï¼šç§»é™¤ç©ºæ ¼ã€æ³¨é‡Šå’Œå˜é‡å
    normalized = re.sub(r'//.*?\n', '', code)  # ç§»é™¤å•è¡Œæ³¨é‡Š
    normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)  # ç§»é™¤å¤šè¡Œæ³¨é‡Š
    normalized = re.sub(r'\s+', '', normalized)  # ç§»é™¤æ‰€æœ‰ç©ºç™½
    normalized = re.sub(r'[a-zA-Z_][a-zA-Z0-9_]*', 'var', normalized)  # æ ‡å‡†åŒ–å˜é‡å
    return hashlib.md5(normalized.encode()).hexdigest()


def prefilter_codes(codes):
    """ä½¿ç”¨å“ˆå¸Œå€¼é¢„ç­›é€‰ç›¸ä¼¼ä»£ç """
    hash_map = defaultdict(list)
    for student, code in codes.items():
        code_hash = calculate_hash(code)
        hash_map[code_hash].append(student)

    # è¿”å›éœ€è¦è¯¦ç»†æ¯”è¾ƒçš„ç»„
    return [group for group in hash_map.values() if len(group) > 1]


def analyze_plagiarism_for_exam(exam_name):
    """åˆ†ææŒ‡å®šè¯„åˆ†çš„æŠ„è¢­æƒ…å†µï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    plagiarism_dir = os.path.join(PLAGIARISM_DIR, exam_name)
    if not os.path.exists(plagiarism_dir):
        return None, "æ²¡æœ‰æ‰¾åˆ°è¯¥è¯„åˆ†çš„æäº¤è®°å½•"

    # è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶
    code_files = [f for f in os.listdir(plagiarism_dir) if f.endswith('.c')]
    if len(code_files) < 2:
        return None, "æäº¤æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒæŠ„è¢­åˆ†æ"

    # è¯»å–æ‰€æœ‰ä»£ç 
    codes = {}
    for file in code_files:
        file_path = os.path.join(plagiarism_dir, file)
        with open(file_path, 'r', encoding='utf-8') as f:
            codes[file] = f.read()

    # 1. å“ˆå¸Œé¢„ç­›é€‰
    hash_groups = prefilter_codes(codes)

    # 2. ä»…å¯¹ç›¸ä¼¼ç»„è¿›è¡Œè¯¦ç»†æ¯”è¾ƒ
    high_similarity_pairs = []

    for group in hash_groups:
        # ä»…å¯¹ç»„å†…å­¦ç”Ÿè¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ
        for i in range(len(group)):
            for j in range(i + 1, len(group)):
                student1 = group[i]
                student2 = group[j]
                similarity = calculate_code_similarity(codes[student1], codes[student2])

                if similarity > 80:
                    high_similarity_pairs.append({
                        "å­¦ç”Ÿ1": student1.replace('.c', ''),
                        "å­¦ç”Ÿ2": student2.replace('.c', ''),
                        "ç›¸ä¼¼åº¦": similarity
                    })

    return high_similarity_pairs, None


def generate_similarity_report(exam_name):
    """ç”ŸæˆæŠ„è¢­æƒ…å†µæŠ¥å‘Š"""
    high_similarity_pairs, error = analyze_plagiarism_for_exam(exam_name)

    if error:
        return None, error

    # åˆ›å»ºæŠ¥å‘Š
    report = {
        "exam_name": exam_name,
        "high_similarity_pairs": high_similarity_pairs,
        "total_pairs": len(high_similarity_pairs)
    }

    return report, None


def ai_generate_exam_config(task_content, api_key):
    """ä½¿ç”¨AIç”Ÿæˆè¯„åˆ†é…ç½®ï¼ˆé’ˆå¯¹main.cæ–‡ä»¶å’Œåˆå­¦è€…ï¼‰"""
    if not api_key:
        return None

    prompt = f"""
ä½ æ˜¯ä¸€ä½åµŒå…¥å¼ç³»ç»Ÿè¯¾ç¨‹ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹é¡¹ç›®è®¾è®¡ä»»åŠ¡ä¹¦ç”Ÿæˆé€‚åˆåˆå­¦è€…çš„è¯„åˆ†é¢˜ç›®é…ç½®ã€‚
è¯·æ³¨æ„ï¼šå­¦ç”Ÿæäº¤çš„æ˜¯main.cæ–‡ä»¶ï¼Œè¯„åˆ†åº”å…³æ³¨main.cä¸­çš„åŠŸèƒ½å®ç°å’Œä»£ç è´¨é‡ã€‚

**ä»»åŠ¡ä¹¦å†…å®¹**:
{task_content[:2500]}  # é™åˆ¶è¾“å…¥é•¿åº¦

**è¯·ç”ŸæˆåŒ…å«1-3é“é¢˜ç›®çš„è¯„åˆ†é…ç½®ï¼Œæ ¼å¼è¦æ±‚**:
{{
  "exam_name": "è¯„åˆ†åç§°",
  "exam_date": "YYYY-MM-DD",
  "questions": [
    {{
      "title": "é¢˜ç›®æ ‡é¢˜",
      "description": "é¢˜ç›®æè¿°",
      "total": åˆ†å€¼,
      "subtasks": [
        {{"desc": "åŠŸèƒ½ç‚¹æè¿°", "score": åˆ†å€¼}},
        ...
      ],
      "code_criteria": ["ä»£ç è´¨é‡è¦æ±‚1", "ä»£ç è´¨é‡è¦æ±‚2"]
    }},
    ...
  ]
}}

**æ³¨æ„äº‹é¡¹**:
1. é¢˜ç›®åº”å…³æ³¨main.cæ–‡ä»¶ä¸­çš„åŠŸèƒ½å®ç°ï¼Œä¸è¦åŒ…å«å¼•è„šå®šä¹‰ç­‰åº•å±‚ç»†èŠ‚
2. é¢˜ç›®æ€»åˆ†è®¾ç½®ä¸º85åˆ†ï¼ˆåŠ ä¸Š15åˆ†ä»£ç è´¨é‡åˆ†ï¼Œæ€»è®¡100åˆ†ï¼‰
3. æ‰€æœ‰é¢˜ç›®éƒ½åº”åŸºäºmain.cæ–‡ä»¶
4. è¯„åˆ†æ ‡å‡†è¦é€‚åˆåˆå­¦è€…ï¼Œéš¾åº¦é€‚ä¸­
5. åŠŸèƒ½ç‚¹åº”å…³æ³¨:
   - æ˜¯å¦æ­£ç¡®è¿›è¡Œåˆå§‹åŒ–
   - ä¸»å¾ªç¯ä¸­çš„åŠŸèƒ½å®ç°
   - æ¨¡å—åŒ–è®¾è®¡ï¼ˆå‡½æ•°åˆ’åˆ†ï¼‰
   - åŸºæœ¬é”™è¯¯å¤„ç†
6. ä»£ç è´¨é‡è¦æ±‚åº”å…³æ³¨:
   - ä»£ç ç»“æ„æ¸…æ™°
   - åŸºæœ¬æ³¨é‡Š
   - å˜é‡å‘½ååˆç†
   - ä»£ç ç®€æ´æ€§
7. é¿å…è¿‡äºä¸¥æ ¼çš„è¦æ±‚ï¼Œè€ƒè™‘åˆå­¦è€…æ°´å¹³

**ç¤ºä¾‹é…ç½®**:
{{
  "exam_name": "LEDé—ªçƒæ§åˆ¶é¡¹ç›®",
  "exam_date": "2023-11-15",
  "questions": [
    {{
      "title": "ä¸»åŠŸèƒ½å®ç°",
      "description": "å®ç°LEDé—ªçƒæ§åˆ¶çš„ä¸»é€»è¾‘",
      "total": 60,
      "subtasks": [
        {{"desc": "æ­£ç¡®åˆå§‹åŒ–ç›¸å…³æ¨¡å—", "score": 15}},
        {{"desc": "å®ç°ä¸»å¾ªç¯ä¸­çš„LEDæ§åˆ¶é€»è¾‘", "score": 25}},
        {{"desc": "å®ç°æŒ‰é”®æ£€æµ‹åŠŸèƒ½", "score": 20}}
      ],
      "code_criteria": ["ä»£ç ç»“æ„æ¸…æ™°", "åŸºæœ¬æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½ååˆç†"]
    }},
    {{
      "title": "å»¶æ—¶åŠŸèƒ½",
      "description": "å®ç°ç²¾ç¡®çš„å»¶æ—¶åŠŸèƒ½",
      "total": 30,
      "subtasks": [
        {{"desc": "å®ç°åŸºæœ¬å»¶æ—¶å‡½æ•°", "score": 15}},
        {{"desc": "åœ¨ä¸»å¾ªç¯ä¸­æ­£ç¡®ä½¿ç”¨å»¶æ—¶", "score": 15}}
      ],
      "code_criteria": ["å‡½æ•°å°è£…åˆç†", "æ— å¿™ç­‰å¾…"]
    }},
    {{
      "title": "ä»£ç è´¨é‡",
      "description": "æ•´ä½“ä»£ç è´¨é‡è¯„ä¼°",
      "total": 10,
      "subtasks": [
        {{"desc": "ä»£ç å¯è¯»æ€§", "score": 5}},
        {{"desc": "åŸºæœ¬é”™è¯¯å¤„ç†", "score": 5}}
      ],
      "code_criteria": ["ä»£ç ç®€æ´", "æ— å†—ä½™ä»£ç "]
    }}
  ]
}}
"""

    try:
        qwen = QwenChat(api_key=api_key)
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„åµŒå…¥å¼ç³»ç»Ÿæ•™å­¦ä¸“å®¶ï¼Œæ“…é•¿ä¸ºåˆå­¦è€…è®¾è®¡åˆç†çš„è¯„åˆ†é¢˜ç›®"),
            HumanMessage(content=prompt)
        ]
        response = qwen.invoke(messages)

        # å°è¯•è§£æJSON
        try:
            config = json.loads(response.content)
        except json.JSONDecodeError:
            # å°è¯•æå–å¯èƒ½çš„JSONéƒ¨åˆ†
            match = re.search(r'\{.*\}', response.content, re.DOTALL)
            if match:
                config = json.loads(match.group(0))
            else:
                raise ValueError("æ— æ³•è§£æAIè¿”å›çš„JSON")

        # éªŒè¯å¹¶è°ƒæ•´é…ç½®
        return validate_and_adjust_config(config)
    except Exception as e:
        st.error(f"AIç”Ÿæˆé…ç½®å¤±è´¥: {str(e)}")
        return None


def validate_and_adjust_config(config):
    """éªŒè¯å¹¶è°ƒæ•´é…ç½®ä½¿å…¶é€‚åˆåˆå­¦è€…"""
    # ç¡®ä¿æ€»åˆ†100
    total_score = sum(q['total'] for q in config['questions'])
    if total_score != 85:
        scale = 85 / total_score
        for q in config['questions']:
            q['total'] = round(q['total'] * scale)

    # è°ƒæ•´é¢˜ç›®æ•°é‡ï¼ˆ1-3é¢˜ï¼‰
    if len(config['questions']) > 3:
        config['questions'] = config['questions'][:3]

    # è°ƒæ•´è¯„åˆ†æ ‡å‡†
    for q in config['questions']:
        # ç®€åŒ–åŠŸèƒ½ç‚¹
        if len(q['subtasks']) > 4:
            q['subtasks'] = q['subtasks'][:4]

        # ç®€åŒ–ä»£ç è´¨é‡è¦æ±‚
        if len(q['code_criteria']) > 3:
            q['code_criteria'] = q['code_criteria'][:3]

        # é™ä½åˆ†æ•°è¦æ±‚
        for subtask in q['subtasks']:
            if subtask['score'] > 20:
                subtask['score'] = 20

    return config
    try:
        qwen = QwenChat(api_key=api_key)
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„åµŒå…¥å¼ç³»ç»Ÿæ•™å­¦ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®é¡¹ç›®ä»»åŠ¡è®¾è®¡è¯„åˆ†é¢˜ç›®"),
            HumanMessage(content=prompt)
        ]
        response = qwen.invoke(messages)

        # å°è¯•è§£æJSON
        try:
            return json.loads(response.content)
        except json.JSONDecodeError:
            # å°è¯•æå–å¯èƒ½çš„JSONéƒ¨åˆ†
            match = re.search(r'\{.*\}', response.content, re.DOTALL)
            if match:
                return json.loads(match.group(0))
            raise
    except Exception as e:
        st.error(f"AIç”Ÿæˆé…ç½®å¤±è´¥: {str(e)}")
        return None


def ai_assistant_score(question, student_code, api_key):
    """AIè¾…åŠ©è¯„åˆ†"""
    if not api_key:
        return "é”™è¯¯: è¯·å…ˆè¾“å…¥APIå¯†é’¥"

    # æ„å»ºæç¤ºè¯
    prompt = f"""
ä½ æ˜¯ä¸€ä½åµŒå…¥å¼ç³»ç»Ÿè¯¾ç¨‹è¯„åˆ†ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹é¢˜ç›®è¦æ±‚è¯„ä¼°å­¦ç”Ÿä»£ç ï¼š

**é¢˜ç›®**: {question['title']}
**æè¿°**: {question['description']}
**åŠŸèƒ½ç‚¹è¦æ±‚**:
"""
    for idx, subtask in enumerate(question['subtasks']):
        prompt += f"    {idx + 1}. {subtask['desc']} (åˆ†å€¼: {subtask['score']}åˆ†)\n"

    prompt += f"""
**ä»£ç è´¨é‡è¦æ±‚**: {', '.join(question['code_criteria'])}

**å­¦ç”Ÿä»£ç **: {student_code[:5000]} 

**è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»™å‡ºè¯„åˆ†å»ºè®®**ï¼š
1. **åŠŸèƒ½ç‚¹å®Œæˆæƒ…å†µ**ï¼ˆæ¯é¡¹åŠŸèƒ½ç‚¹å•ç‹¬è¯„ä¼°ï¼‰ï¼š
   - åŠŸèƒ½ç‚¹1: [å®ç°æƒ…å†µæè¿°] (å¾—åˆ†: x/y)
   - åŠŸèƒ½ç‚¹2: [å®ç°æƒ…å†µæè¿°] (å¾—åˆ†: x/y)
   ...
2. **ä»£ç è´¨é‡è¯„ä¼°**ï¼š
   - ä¼˜ç‚¹: [åˆ—å‡ºä»£ç çš„ä¼˜ç‚¹]
   - æ”¹è¿›å»ºè®®: [åˆ—å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹]
3. **æ€»ä½“è¯„ä»·ä¸å»ºè®®**:

ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚
"""

    try:
        qwen = QwenChat(api_key=api_key)
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„åµŒå…¥å¼ç³»ç»Ÿå·¥ç¨‹å¸ˆï¼Œæ“…é•¿è¯„ä¼°å­¦ç”Ÿä»£ç è´¨é‡"),
            HumanMessage(content=prompt)
        ]
        response = qwen.invoke(messages)
        return response.content
    except Exception as e:
        return f"AIè¯„åˆ†å¤±è´¥: {str(e)}"


def create_exam_config_ui():
    """åˆ›å»ºè¯„åˆ†é…ç½®ç•Œé¢"""
    st.header("ğŸ“ åˆ›å»ºè¯„åˆ†é…ç½®")

    # APIå¯†é’¥è®¾ç½®
    api_key = st.sidebar.text_input("AI APIå¯†é’¥", type="password",
                                    value=st.session_state.get('api_key', ''))
    st.session_state.api_key = api_key

    # ç¡®ä¿ exam_config å­˜åœ¨
    if 'exam_config' not in st.session_state or st.session_state.exam_config is None:
        st.session_state.exam_config = {
            'exam_name': '',
            'exam_date': '',
            'questions': [],
            'code_criteria': ["ä»£ç ç»“æ„æ¸…æ™°", "æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½ååˆç†"],
            'code_scores': [5, 5, 5]  # é»˜è®¤åˆ†æ•°
        }

    config = st.session_state.exam_config
    questions = config.get('questions', [])

    # é¡¹ç›®è®¾è®¡ä»»åŠ¡ä¹¦ä¸Šä¼  - ä¿ç•™AIç”ŸæˆåŠŸèƒ½
    st.subheader("1. ä¸Šä¼ é¡¹ç›®è®¾è®¡ä»»åŠ¡ä¹¦")
    uploaded_task = st.file_uploader("ä¸Šä¼ PDF/DOCXä»»åŠ¡ä¹¦", type=['pdf', 'docx'])

    if uploaded_task is not None:
        try:
            # ç®€åŒ–å¤„ç†ï¼šå®é™…åº”ä½¿ç”¨PyPDF2æˆ–python-docxæå–æ–‡æœ¬
            task_content = f"ä¸Šä¼ æ–‡ä»¶: {uploaded_task.name} (å†…å®¹æå–éœ€å®é™…å®ç°)"
            st.session_state.design_task = task_content
            st.success("ä»»åŠ¡ä¹¦å·²ä¸Šä¼ !")
        except Exception as e:
            st.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")

    # AIç”Ÿæˆé…ç½® - ä¿ç•™AIç”ŸæˆåŠŸèƒ½
    if st.button("ğŸ¤– AIç”Ÿæˆè¯„åˆ†é…ç½®", disabled=not st.session_state.get('design_task', None)):
        with st.spinner("AIæ­£åœ¨ç”Ÿæˆè¯„åˆ†é…ç½®..."):
            config = ai_generate_exam_config(
                st.session_state.design_task,
                st.session_state.api_key
            )
            if config:
                st.session_state.exam_config = config
                st.success("è¯„åˆ†é…ç½®ç”ŸæˆæˆåŠŸ!")
                st.rerun()  # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºæ–°é…ç½®

    # æ‰‹åŠ¨é…ç½®åŒºåŸŸ
    st.subheader("2. è°ƒæ•´è¯„åˆ†é…ç½®")
    if not st.session_state.get('exam_config', None):
        st.warning("è¯·å…ˆä¸Šä¼ ä»»åŠ¡ä¹¦å¹¶ç”Ÿæˆé…ç½®")
        return None

    config = st.session_state.exam_config
    questions = config.get('questions', [])

    # åŸºç¡€ä¿¡æ¯ç¼–è¾‘
    col1, col2 = st.columns(2)
    with col1:
        exam_name = st.text_input("è¯„åˆ†åç§°", value=config.get('exam_name', ''))
    with col2:
        exam_date = st.text_input("è¯„åˆ†æ—¥æœŸ", value=config.get('exam_date', ''))

    # æ›´æ–°åŸºç¡€ä¿¡æ¯åˆ°session_state
    st.session_state.exam_config['exam_name'] = exam_name
    st.session_state.exam_config['exam_date'] = exam_date

    # å…¨å±€ä»£ç è´¨é‡è¦æ±‚
    st.subheader("å…¨å±€ä»£ç è´¨é‡è¦æ±‚")
    code_criteria = config.get('code_criteria', ["ä»£ç ç»“æ„æ¸…æ™°", "æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½ååˆç†"])
    code_scores = config.get('code_scores', [5, 5, 5])

    # ç¡®ä¿code_scoresé•¿åº¦ä¸code_criteriaåŒ¹é…
    if len(code_scores) < len(code_criteria):
        code_scores.extend([5] * (len(code_criteria) - len(code_scores)))

    for j, criterion in enumerate(code_criteria):
        col1, col2 = st.columns([3, 1])
        with col1:
            crit_desc = st.text_input(f"è¦æ±‚ {j + 1}", value=criterion, key=f"crit_{j}_desc")
        with col2:
            crit_score = st.number_input("åˆ†å€¼", 1, 100,
                                         value=code_scores[j] if j < len(code_scores) else 5,
                                         key=f"crit_{j}_score")

        # æ›´æ–°åˆ—è¡¨
        if j < len(code_criteria):
            code_criteria[j] = crit_desc
        if j < len(code_scores):
            code_scores[j] = crit_score

    # æ·»åŠ /åˆ é™¤è´¨é‡è¦æ±‚
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ·»åŠ è¦æ±‚", key="add_crit"):
            code_criteria.append("æ–°è¦æ±‚")
            code_scores.append(5)
            st.session_state.exam_config['code_criteria'] = code_criteria
            st.session_state.exam_config['code_scores'] = code_scores
            st.rerun()  # åˆ·æ–°é¡µé¢
    with col2:
        if len(code_criteria) > 1 and st.button("â– åˆ é™¤è¦æ±‚", key="del_crit"):
            code_criteria.pop()
            code_scores.pop()
            st.session_state.exam_config['code_criteria'] = code_criteria
            st.session_state.exam_config['code_scores'] = code_scores
            st.rerun()  # åˆ·æ–°é¡µé¢

    # ä¿å­˜ä»£ç è´¨é‡åˆ†æ•°åˆ°session_state
    st.session_state.exam_config['code_criteria'] = code_criteria
    st.session_state.exam_config['code_scores'] = code_scores

    # é¢˜ç›®é…ç½®ç¼–è¾‘
    st.subheader("é¢˜ç›®é…ç½®")

    # é¢˜ç›®ç¼–è¾‘å™¨
    for i, q in enumerate(questions):
        with st.expander(f"é¢˜ç›® {i + 1}: {q['title']}", expanded=True):
            col1, col2 = st.columns([3, 1])
            with col1:
                title = st.text_input("æ ‡é¢˜", value=q['title'], key=f"q{i}_title")
            with col2:
                total = st.number_input("æ€»åˆ†", 1, 100, value=q['total'], key=f"q{i}_total")

            description = st.text_area("æè¿°", value=q['description'],
                                       height=100, key=f"q{i}_desc")

            # æ›´æ–°é¢˜ç›®ä¿¡æ¯åˆ°session_state
            st.session_state.exam_config['questions'][i]['title'] = title
            st.session_state.exam_config['questions'][i]['total'] = total
            st.session_state.exam_config['questions'][i]['description'] = description

            # åŠŸèƒ½ç‚¹é…ç½®
            st.markdown("**åŠŸèƒ½ç‚¹**")
            subtasks = q.get('subtasks', [])

            for j, subtask in enumerate(subtasks):
                col1, col2 = st.columns([4, 1])
                with col1:
                    desc = st.text_input(f"åŠŸèƒ½ç‚¹ {j + 1} æè¿°",
                                         value=subtask['desc'],
                                         key=f"q{i}_sub{j}_desc")
                with col2:
                    score = st.number_input("åˆ†å€¼", 1, total,
                                            value=subtask['score'],
                                            key=f"q{i}_sub{j}_score")

                # æ›´æ–°åŠŸèƒ½ç‚¹ä¿¡æ¯åˆ°session_state
                st.session_state.exam_config['questions'][i]['subtasks'][j]['desc'] = desc
                st.session_state.exam_config['questions'][i]['subtasks'][j]['score'] = score

            # æ·»åŠ /åˆ é™¤åŠŸèƒ½ç‚¹
            col1, col2 = st.columns(2)
            with col1:
                if st.button("â• æ·»åŠ åŠŸèƒ½ç‚¹", key=f"q{i}_add_sub"):
                    st.session_state.exam_config['questions'][i]['subtasks'].append({"desc": "æ–°åŠŸèƒ½ç‚¹", "score": 5})
                    st.rerun()  # åˆ·æ–°é¡µé¢
            with col2:
                if len(subtasks) > 1 and st.button("â– åˆ é™¤åŠŸèƒ½ç‚¹", key=f"q{i}_del_sub"):
                    st.session_state.exam_config['questions'][i]['subtasks'].pop()
                    st.rerun()  # åˆ·æ–°é¡µé¢

    # æ·»åŠ /åˆ é™¤é¢˜ç›®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â• æ·»åŠ æ–°é¢˜ç›®"):
            st.session_state.exam_config['questions'].append({
                "title": f"é¢˜ç›® {len(questions) + 1}",
                "description": "",
                "total": 20,
                "subtasks": [{"desc": "ä¸»è¦åŠŸèƒ½", "score": 10}]
            })
            st.rerun()  # åˆ·æ–°é¡µé¢
    with col2:
        if len(questions) > 1 and st.button("â– åˆ é™¤é¢˜ç›®"):
            st.session_state.exam_config['questions'].pop()
            st.rerun()  # åˆ·æ–°é¡µé¢

    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    if st.button("ğŸ’¾ ä¿å­˜è¯„åˆ†é…ç½®"):
        config = {
            "exam_name": exam_name,
            "exam_date": exam_date,
            "questions": st.session_state.exam_config['questions'],
            "code_criteria": code_criteria,
            "code_scores": code_scores,
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M")
        }

        filename = f"{exam_name}_{exam_date}.json"
        filepath = os.path.join(CONFIG_DIR, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        st.session_state.exam_config = config
        st.success(f"é…ç½®å·²ä¿å­˜: {filepath}")
        return config

    return None


def load_exam_config_ui():
    """åŠ è½½è¯„åˆ†é…ç½®ç•Œé¢"""
    st.header("ğŸ“‚ åŠ è½½è¯„åˆ†é…ç½®")

    # è·å–æ‰€æœ‰é…ç½®æ–‡ä»¶
    config_files = [f for f in os.listdir(CONFIG_DIR) if f.endswith(".json")]
    if not config_files:
        st.warning("æ²¡æœ‰æ‰¾åˆ°è¯„åˆ†é…ç½®æ–‡ä»¶")
        return None

    selected_file = st.selectbox("é€‰æ‹©è¯„åˆ†é…ç½®", config_files)
    filepath = os.path.join(CONFIG_DIR, selected_file)

    if st.button("åŠ è½½é…ç½®"):
        with open(filepath, "r", encoding='utf-8') as f:
            config = json.load(f)
            st.session_state.exam_config = config
            st.success(f"å·²åŠ è½½é…ç½®: {config['exam_name']}")
            return config
    return None


def scoring_interface(config):
    """è¯„åˆ†ç•Œé¢"""
    if config is None:
        st.error("è¯„åˆ†é…ç½®æœªåŠ è½½ï¼")
        return

    st.header(f"ğŸ“ è¯„åˆ† - {config['exam_name']}")
    st.caption(f"è¯„åˆ†æ—¥æœŸ: {config['exam_date']}")

    # åœ¨ä¾§è¾¹æ è®¾ç½®APIå¯†é’¥
    api_key = st.sidebar.text_input("è¾“å…¥AIè¯„åˆ†APIå¯†é’¥", type="password",
                                    value=st.session_state.get('api_key', ''),
                                    help="ä»é˜¿é‡Œäº‘DashScopeå¹³å°è·å–")
    st.session_state.api_key = api_key

    # å­¦ç”Ÿä¿¡æ¯
    st.subheader("å­¦ç”Ÿä¿¡æ¯")
    col1, col2 = st.columns(2)
    with col1:
        student_id = st.text_input("å­¦å·", value=st.session_state.get('student_id', ''))
    with col2:
        student_name = st.text_input("å§“å", value=st.session_state.get('student_name', ''))
    st.session_state.student_id = student_id
    st.session_state.student_name = student_name

    # ä»£ç ä¸Šä¼ 
    st.subheader("ä»£ç æäº¤")
    uploaded_file = st.file_uploader("ä¸Šä¼ å­¦ç”Ÿä»£ç ", type=['c', 'cpp', 'h'])
    code_content = st.session_state.get('student_code', "")

    if uploaded_file is not None:
        try:
            code_content = uploaded_file.getvalue().decode("utf-8")  # å°è¯•UTF-8è§£ç 
        except UnicodeDecodeError:
            try:
                code_content = uploaded_file.getvalue().decode("gbk")  # å°è¯•GBKè§£ç  (å¸¸è§äºä¸­æ–‡Windows)
                st.warning("ä»£ç æ–‡ä»¶ä¼¼ä¹ä½¿ç”¨GBKç¼–ç ï¼Œå·²å°è¯•è½¬æ¢ã€‚è¯·ç¡®ä¿å†…å®¹æ­£ç¡®ã€‚")
            except UnicodeDecodeError:
                st.error("æ— æ³•è§£ç ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ–‡ä»¶æ˜¯æ–‡æœ¬æ ¼å¼ï¼ˆå¦‚ .c, .cpp, .hï¼‰å¹¶ä½¿ç”¨UTF-8æˆ–GBKç¼–ç ã€‚")
                code_content = ""  # è§£ç å¤±è´¥åˆ™ç½®ç©º

        if code_content:
            st.session_state.student_code = code_content
            with st.expander("æŸ¥çœ‹ä»£ç ", expanded=False):
                # ç®€å•åˆ¤æ–­è¯­è¨€ï¼ŒStreamlitä¼šè‡ªåŠ¨å¤„ç†
                language = 'cpp' if uploaded_file.name.endswith(('.cpp', '.h')) else 'c'
                st.code(code_content, language=language)

    # è¯„åˆ†åŒºåŸŸ
    st.subheader("è¯„åˆ†")
    total_score = 0
    scores = {}
    comments = {}

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    num_questions = len(config.get('questions', []))

    if num_questions == 0:
        st.warning("è¯„åˆ†é…ç½®ä¸­æ²¡æœ‰é¢˜ç›®")
        return

    # åŠŸèƒ½ç‚¹è¯„åˆ†éƒ¨åˆ†
    for i, q in enumerate(config['questions']):
        # æ›´æ–°è¿›åº¦æ¡
        progress_value = (i + 1) / num_questions if num_questions > 0 else 0
        progress_bar.progress(progress_value)

        with st.expander(f"{q['title']} - {q['total']}åˆ†", expanded=(i == 0)):
            # AIè¾…åŠ©è¯„åˆ†æŒ‰é’®
            if code_content and st.button(f"ğŸ¤– AIè¾…åŠ©è¯„åˆ† - {q['title']}", key=f"ai_{i}", use_container_width=True):
                with st.spinner("AIè¯„åˆ†ä¸­..."):
                    feedback = ai_assistant_score(q, st.session_state.student_code, api_key)
                    st.session_state.ai_feedback[q['title']] = feedback

            # æ˜¾ç¤ºAIåé¦ˆ
            if q['title'] in st.session_state.get('ai_feedback', {}):
                st.subheader("ğŸ¤– AIè¯„åˆ†åé¦ˆ")
                st.info(st.session_state.ai_feedback[q['title']])
                st.divider()

            # åŠŸèƒ½ç‚¹è¯„åˆ†
            st.markdown(f"**åŠŸèƒ½å®ç° ({q['total']}åˆ†)**")
            func_score = 0
            q_comments = []

            for j, subtask in enumerate(q['subtasks']):
                col1, col2, col3 = st.columns([0.4, 0.3, 0.3])
                with col1:
                    st.markdown(f"**{subtask['desc']}**")
                with col2:
                    status = st.selectbox(
                        "å®Œæˆæƒ…å†µ",
                        ["æœªå®ç°", "éƒ¨åˆ†å®ç°", "å®Œå…¨å®ç°"],
                        index=["æœªå®ç°", "éƒ¨åˆ†å®ç°", "å®Œå…¨å®ç°"].index(
                            st.session_state.get(f"q{i}_sub{j}_status", "æœªå®ç°")),
                        key=f"q{i}_sub{j}_status"
                    )
                with col3:
                    max_score = float(subtask['score'])
                    if status == "æœªå®ç°":
                        score = 0.0
                    elif status == "éƒ¨åˆ†å®ç°":
                        default_partial = max_score / 2.0
                        score = st.number_input(
                            "å¾—åˆ†",
                            0.0, max_score, st.session_state.get(f"q{i}_sub{j}_score", default_partial),
                            key=f"q{i}_sub{j}_score",
                            step=0.5
                        )
                    else:  # å®Œå…¨å®ç°
                        score = max_score
                    st.markdown(f"**å¾—åˆ†: {score:.1f}/{max_score}**")

                # è¯„è¯­
                comment = st.text_area("è¯„è¯­", value=st.session_state.get(f"q{i}_sub{j}_comment", ""),
                                       key=f"q{i}_sub{j}_comment", height=60,
                                       placeholder="è®°å½•å®ç°ç»†èŠ‚ã€é—®é¢˜æˆ–å»ºè®®...")
                if comment:
                    q_comments.append(f"{subtask['desc']}: {comment}")

                func_score += score

            # é¢˜ç›®æ€»åˆ†
            st.markdown(f"**é¢˜ç›®å¾—åˆ†: {func_score:.1f}/{q['total']}**")
            st.markdown("---")

            total_score += func_score
            scores[q['title']] = func_score
            comments[q['title']] = q_comments

    # å…¨å±€ä»£ç è´¨é‡è¯„åˆ†
    st.subheader("ä»£ç è´¨é‡è¯„åˆ†")
    code_criteria = config.get('code_criteria', [])
    code_scores = config.get('code_scores', [])
    code_total = sum(code_scores) if code_scores else 0

    if code_total > 0:
        st.markdown(f"**ä»£ç è´¨é‡ ({code_total}åˆ†)**")
        code_score = 0
        code_comments = []

        # å¦‚æœæœ‰é…ç½®çš„ä»£ç è´¨é‡è¦æ±‚
        if code_criteria and code_scores:
            for j, (criterion, max_score) in enumerate(zip(code_criteria, code_scores)):
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.markdown(f"**{criterion}**")
                with col2:
                    score_val = st.number_input(
                        "å¾—åˆ†",
                        0.0, float(max_score),
                        st.session_state.get(f"code_crit{j}_score", max_score * 0.75),
                        key=f"code_crit{j}_score",
                        step=0.5
                    )
                    st.markdown(f"**å¾—åˆ†: {score_val:.1f}/{max_score}**")

                # è¯„è¯­
                comment = st.text_area(f"{criterion}è¯„è¯­",
                                       value=st.session_state.get(f"code_crit{j}_comment", ""),
                                       key=f"code_crit{j}_comment", height=60,
                                       placeholder="è®°å½•ä»£ç è´¨é‡è¯„ä¼°...")
                if comment:
                    code_comments.append(f"{criterion}: {comment}")

                code_score += score_val
        else:
            st.warning("âš ï¸ æœªé…ç½®ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†")
            code_score = st.slider(
                "ä»£ç è´¨é‡è¯„åˆ†",
                0.0, 20.0, st.session_state.get("code_score", 15.0),
                key="code_score",
                step=0.5
            )

            code_comment = st.text_area("ä»£ç è´¨é‡è¯„è¯­", value=st.session_state.get("code_comment", ""),
                                        key="code_comment", height=80,
                                        placeholder="è®°å½•ä»£ç ç»“æ„ã€é£æ ¼ã€ä¼˜åŒ–å»ºè®®ç­‰...")
            if code_comment:
                code_comments.append(f"ä»£ç è´¨é‡: {code_comment}")

        # ä»£ç è´¨é‡åˆ†æ
        if code_content:
            analysis = analyze_code(code_content)

            st.caption("ä»£ç åˆ†æç»“æœ:")
            col1, col2, col3 = st.columns(3)
            col1.metric("ä»£ç è¡Œæ•°", analysis["line_count"])
            col2.metric("æ³¨é‡Šæ•°é‡", analysis["comment_count"])
            col3.metric("æ³¨é‡Šæ¯”ä¾‹", f"{analysis['comment_ratio']:.1f}%")

            col1, col2 = st.columns(2)
            col1.metric("å‡½æ•°æ•°é‡", analysis["function_count"])
            col2.metric("å¹³å‡å‡½æ•°é•¿åº¦", f"{analysis['avg_function_length']:.1f}è¡Œ")

            if "issues" in analysis and analysis["issues"]:
                st.warning("æ½œåœ¨é—®é¢˜æ£€æµ‹")
                for issue in analysis["issues"]:
                    st.write(issue)

        st.markdown(f"**ä»£ç è´¨é‡å¾—åˆ†: {code_score:.1f}/{code_total}**")
        st.markdown("---")

        total_score += code_score
        scores["ä»£ç è´¨é‡"] = code_score
        comments["ä»£ç è´¨é‡"] = code_comments
    else:
        st.warning("âš ï¸ æœªé…ç½®ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†")

    st.session_state.scores = scores
    st.session_state.comments = comments

    # ç»“æœå¯è§†åŒ–
    st.subheader("æˆç»©æ¦‚è§ˆ")
    st.metric("æ€»åˆ†", f"{total_score:.1f}")

    # æœ€ç»ˆæäº¤
    st.subheader("æäº¤è¯„åˆ†")
    if st.button("âœ… æäº¤è¯„åˆ†", use_container_width=True, type="primary"):
        st.session_state.total_score = total_score

        # ä¿å­˜è¯„åˆ†ç»“æœ
        if student_id and student_name:
            try:
                # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
                result_file = save_results(
                    student_id, student_name, config,
                    st.session_state.scores,
                    st.session_state.comments,
                    st.session_state.get('ai_feedback', {}),
                    code_content
                )

                st.success(f"âœ… è¯„åˆ†ç»“æœå·²ä¿å­˜è‡³: {result_file}", icon="ğŸ‰")

            except Exception as e:
                st.error(f"âŒ ä¿å­˜è¯„åˆ†ç»“æœæ—¶å‡ºé”™: {e}")
        else:
            st.warning("âš ï¸ è¯·å¡«å†™å­¦å·å’Œå§“ååå†æäº¤ã€‚")

def save_results(student_id, student_name, config, scores, comments, ai_feedback, code_content):
    """ä¿å­˜è¯„åˆ†ç»“æœ"""
    result = {
        "student_id": student_id,
        "student_name": student_name,
        "exam_name": config['exam_name'],
        "exam_date": config['exam_date'],
        "score_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "total_score": sum(scores.values()),
        "scores": scores,
        "comments": comments,
        "ai_feedback": ai_feedback
    }

    # åˆ›å»ºå­¦ç”Ÿç›®å½•
    student_dir = f"{student_id}_{student_name}"
    os.makedirs(student_dir, exist_ok=True)

    # ä¿å­˜ç»“æœåˆ°å­¦ç”Ÿç›®å½•
    student_result_file = os.path.join(student_dir, f"{config['exam_name']}_result.json")
    with open(student_result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    # åŒæ—¶ä¿å­˜åˆ°ç»Ÿä¸€ç»“æœç›®å½•
    result_file = os.path.join(RESULTS_DIR, f"{student_id}_{student_name}_{config['exam_name']}_result.json")
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    # ä¿å­˜ä»£ç 
    if code_content:
        code_file = os.path.join(student_dir, f"{config['exam_name']}_code.c")
        with open(code_file, 'w', encoding='utf-8') as f:
            f.write(code_content)

        # åŒæ—¶ä¿å­˜åˆ°æŠ„è¢­æ£€æµ‹ç›®å½•
        plagiarism_dir = os.path.join(PLAGIARISM_DIR, config['exam_name'])
        os.makedirs(plagiarism_dir, exist_ok=True)
        plagiarism_file = os.path.join(plagiarism_dir, f"{student_id}_{student_name}.c")
        with open(plagiarism_file, 'w', encoding='utf-8') as f:
            f.write(code_content)

    return result_file


def show_learning_feedback():
    """æ˜¾ç¤ºå­¦æƒ…åé¦ˆç•Œé¢"""
    st.header("ğŸ“Š å­¦æƒ…åé¦ˆ")

    # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½è¯„åˆ†é…ç½®
    if st.session_state.exam_config is None:
        st.warning("è¯·å…ˆåŠ è½½æˆ–åˆ›å»ºä¸€ä¸ªè¯„åˆ†é…ç½®ï¼")
        return

    # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•´ä½“è¡¨ç°
    st.subheader("ç­çº§æ•´ä½“è¡¨ç°")

    # ä»ç»“æœç›®å½•åŠ è½½æ‰€æœ‰å­¦ç”Ÿæˆç»©
    if not os.path.exists(RESULTS_DIR):
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„åˆ†ç»“æœæ•°æ®")
        return

    result_files = [f for f in os.listdir(RESULTS_DIR) if f.endswith(".json")]
    if not result_files:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„åˆ†ç»“æœæ–‡ä»¶")
        return

    # è·å–æ‰€æœ‰è¯„åˆ†åç§°
    exam_names = list(set([f.split('_')[2] for f in result_files if '_' in f]))
    if not exam_names:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„åˆ†åç§°")
        return

    selected_exam = st.selectbox("é€‰æ‹©è¯„åˆ†", exam_names)

    # åŠ è½½è¯¥è¯„åˆ†çš„æ‰€æœ‰ç»“æœ
    exam_results = []
    for file in result_files:
        if selected_exam in file:
            filepath = os.path.join(RESULTS_DIR, file)
            with open(filepath, 'r', encoding='utf-8') as f:
                result = json.load(f)
                exam_results.append(result)

    if not exam_results:
        st.warning(f"æ²¡æœ‰æ‰¾åˆ°'{selected_exam}'çš„è¯„åˆ†ç»“æœ")
        return

    # æå–å­¦ç”Ÿæˆç»©æ•°æ®
    students = []
    scores = []
    for result in exam_results:
        students.append(f"{result['student_id']}_{result['student_name']}")
        scores.append(result['total_score'])

    avg_score = np.mean(scores) if scores else 0
    max_score = max(scores) if scores else 0
    min_score = min(scores) if scores else 0

    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    col1, col2, col3 = st.columns(3)
    col1.metric("å¹³å‡åˆ†", f"{avg_score:.1f}")
    col2.metric("æœ€é«˜åˆ†", max_score)
    col3.metric("æœ€ä½åˆ†", min_score)

    # æ˜¾ç¤ºå­¦ç”Ÿæˆç»©æŸ±çŠ¶å›¾
    st.write("å­¦ç”Ÿæˆç»©åˆ†å¸ƒ:")
    chart_data = pd.DataFrame({'å­¦ç”Ÿ': students, 'åˆ†æ•°': scores})

    chart = alt.Chart(chart_data).mark_bar().encode(
        x=alt.X('å­¦ç”Ÿ', sort=None),
        y='åˆ†æ•°',
        color=alt.value('skyblue')
    ).properties(width=600, height=300)

    # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
    text = chart.mark_text(
        align='center',
        baseline='bottom',
        dy=-5  # è°ƒæ•´æ–‡æœ¬ä½ç½®
    ).encode(
        text='åˆ†æ•°'
    )

    st.altair_chart(chart + text)

    # æˆç»©åˆ†å¸ƒåˆ†æ
    st.subheader("æˆç»©åˆ†å¸ƒåˆ†æ")
    st.write(f"- ä¼˜ç§€ (â‰¥90åˆ†): {len([s for s in scores if s >= 90])}äºº")
    st.write(f"- è‰¯å¥½ (80-89åˆ†): {len([s for s in scores if 80 <= s < 90])}äºº")
    st.write(f"- ä¸­ç­‰ (70-79åˆ†): {len([s for s in scores if 70 <= s < 80])}äºº")
    st.write(f"- åŠæ ¼ (60-69åˆ†): {len([s for s in scores if 60 <= s < 70])}äºº")
    st.write(f"- ä¸åŠæ ¼ (<60åˆ†): {len([s for s in scores if s < 60])}äºº")

    # æ–°å¢ï¼šç­çº§å¼ºé¡¹å’Œå¼±é¡¹åˆ†æ
    st.subheader("ç­çº§å¼ºé¡¹ä¸å¼±é¡¹åˆ†æ")

    # æ”¶é›†æ‰€æœ‰é¢˜ç›®çš„å¹³å‡å¾—åˆ†ç‡
    topic_scores = {}
    topic_counts = {}

    for result in exam_results:
        for topic, score in result['scores'].items():
            # æ‰¾åˆ°è¯¥é¢˜ç›®çš„æ€»åˆ†
            for q in st.session_state.exam_config.get('questions', []):
                if q['title'] == topic:
                    total = q['total']
                    break
            else:
                total = 100  # é»˜è®¤å€¼

            # è®¡ç®—å¾—åˆ†ç‡
            score_rate = (score / total) * 100

            if topic not in topic_scores:
                topic_scores[topic] = 0
                topic_counts[topic] = 0

            topic_scores[topic] += score_rate
            topic_counts[topic] += 1

    # è®¡ç®—å¹³å‡å¾—åˆ†ç‡
    topics = []
    avg_topic_scores = []
    for topic, total_score in topic_scores.items():
        count = topic_counts[topic]
        avg_score = total_score / count
        topics.append(topic)
        avg_topic_scores.append(avg_score)

    # è®¡ç®—å¼ºé¡¹å’Œå¼±é¡¹
    strong_topics = []
    weak_topics = []

    for i, score in enumerate(avg_topic_scores):
        if score >= 85:
            strong_topics.append(topics[i])
        elif score < 70:
            weak_topics.append(topics[i])

    # æ˜¾ç¤ºå¼ºé¡¹å’Œå¼±é¡¹
    col1, col2 = st.columns(2)

    with col1:
        st.success("**ç­çº§å¼ºé¡¹**")
        if strong_topics:
            for topic in strong_topics:
                st.write(f"- {topic} (å¾—åˆ†ç‡: {avg_topic_scores[topics.index(topic)]:.1f}%)")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼ºé¡¹")

    with col2:
        st.warning("**ç­çº§å¼±é¡¹**")
        if weak_topics:
            for topic in weak_topics:
                st.write(f"- {topic} (å¾—åˆ†ç‡: {avg_topic_scores[topics.index(topic)]:.1f}%)")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼±é¡¹")

    # é¢˜ç›®å¾—åˆ†ç‡å¯è§†åŒ–
    st.subheader("å„é¢˜ç›®å¾—åˆ†ç‡")
    topic_df = pd.DataFrame({'é¢˜ç›®': topics, 'å¹³å‡å¾—åˆ†ç‡': avg_topic_scores})

    # åˆ›å»ºæŸ±çŠ¶å›¾ - ä½¿ç”¨æ­£ç¡®çš„æ¡ä»¶é¢œè‰²è¯­æ³•
    chart = alt.Chart(topic_df).mark_bar().encode(
        x=alt.X('é¢˜ç›®', sort=None, axis=alt.Axis(labelAngle=45)),
        y=alt.Y('å¹³å‡å¾—åˆ†ç‡', scale=alt.Scale(domain=[0, 100])),
        color=alt.Color('å¹³å‡å¾—åˆ†ç‡:Q',
                        scale=alt.Scale(
                            domain=[0, 70, 85, 100],
                            range=['red', 'skyblue', 'green', 'green']
                        ),
                        legend=None)
    ).properties(width=600, height=400)

    # æ·»åŠ å‚è€ƒçº¿
    rule_85 = alt.Chart(pd.DataFrame({'y': [85]})).mark_rule(color='green', strokeDash=[5, 5]).encode(y='y')
    rule_70 = alt.Chart(pd.DataFrame({'y': [70]})).mark_rule(color='red', strokeDash=[5, 5]).encode(y='y')

    # æ·»åŠ æ–‡æœ¬æ ‡ç­¾
    text = chart.mark_text(
        align='center',
        baseline='bottom',
        dy=-5
    ).encode(
        text=alt.Text('å¹³å‡å¾—åˆ†ç‡:Q', format='.1f')
    )

    st.altair_chart(chart + rule_85 + rule_70 + text)

    # ç¬¬äºŒéƒ¨åˆ†ï¼šä¸ªäººåˆ†æ•°
    st.subheader("ä¸ªäººåˆ†æ•°åˆ†æ")
    selected_student = st.selectbox("é€‰æ‹©å­¦ç”Ÿ", students)

    # æŸ¥æ‰¾è¯¥å­¦ç”Ÿçš„è¯¦ç»†ç»“æœ
    student_result = None
    for result in exam_results:
        if f"{result['student_id']}_{result['student_name']}" == selected_student:
            student_result = result
            break

    if not student_result:
        st.warning("æ‰¾ä¸åˆ°è¯¥å­¦ç”Ÿçš„è¯¦ç»†ç»“æœ")
        return

    st.metric(f"{selected_student}çš„åˆ†æ•°", f"{student_result['total_score']}åˆ†")

    # æ˜¾ç¤ºå­¦ç”Ÿå…·ä½“è¡¨ç°
    st.write("å…·ä½“è¡¨ç°:")
    col1, col2 = st.columns(2)
    with col1:
        st.info("**å¼ºé¡¹**")
        # æ‰¾å‡ºå¾—åˆ†ç‡é«˜çš„é¢˜ç›®
        strong_topics = []
        for topic, score in student_result['scores'].items():
            # æ‰¾åˆ°è¯¥é¢˜ç›®çš„æ€»åˆ†
            for q in st.session_state.exam_config.get('questions', []):
                if q['title'] == topic:
                    total = q['total']
                    break
            else:
                total = 100  # é»˜è®¤å€¼

            # è®¡ç®—å¾—åˆ†ç‡
            score_rate = (score / total) * 100
            if score_rate >= 85:
                strong_topics.append(f"{topic} ({score_rate:.1f}%)")

        if strong_topics:
            for topic in strong_topics:
                st.write(f"- {topic}")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼ºé¡¹")

    with col2:
        st.warning("**å¼±é¡¹**")
        # æ‰¾å‡ºå¾—åˆ†ç‡ä½çš„é¢˜ç›®
        weak_topics = []
        for topic, score in student_result['scores'].items():
            # æ‰¾åˆ°è¯¥é¢˜ç›®çš„æ€»åˆ†
            for q in st.session_state.exam_config.get('questions', []):
                if q['title'] == topic:
                    total = q['total']
                    break
            else:
                total = 100  # é»˜è®¤å€¼

            # è®¡ç®—å¾—åˆ†ç‡
            score_rate = (score / total) * 100
            if score_rate < 70:
                weak_topics.append(f"{topic} ({score_rate:.1f}%)")

        if weak_topics:
            for topic in weak_topics:
                st.write(f"- {topic}")
        else:
            st.write("æš‚æ— æ˜¾è‘—å¼±é¡¹")


# --- æŠ„è¢­æƒ…å†µç•Œé¢ ---
def show_plagiarism_report():
    """æ˜¾ç¤ºæŠ„è¢­æƒ…å†µæŠ¥å‘Š"""
    st.header("ğŸ” æŠ„è¢­æƒ…å†µåˆ†æ")

    # è·å–æ‰€æœ‰ä½œä¸šï¼ˆè¯„åˆ†ï¼‰åç§°
    exam_names = [d for d in os.listdir(PLAGIARISM_DIR) if os.path.isdir(os.path.join(PLAGIARISM_DIR, d))]

    if not exam_names:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä½œä¸šçš„æŠ„è¢­æ•°æ®")
        return

    # é€‰æ‹©ä½œä¸š
    selected_exam = st.selectbox("é€‰æ‹©ä½œä¸š", exam_names)

    if st.button("åˆ†ææŠ„è¢­æƒ…å†µ"):
        with st.spinner("æ­£åœ¨åˆ†ææŠ„è¢­æƒ…å†µ..."):
            report, error = generate_similarity_report(selected_exam)

        if error:
            st.warning(error)
            return

        st.subheader(f"ä½œä¸š: {selected_exam}")
        st.caption(f"é«˜ç›¸ä¼¼åº¦é…å¯¹æ•°é‡: {report['total_pairs']}")

        # æ˜¾ç¤ºé«˜ç›¸ä¼¼åº¦é…å¯¹
        if report['high_similarity_pairs']:
            st.subheader("é«˜ç›¸ä¼¼åº¦é…å¯¹ (ç›¸ä¼¼åº¦ > 80%)")
            for pair in report['high_similarity_pairs']:
                st.warning(f"âš ï¸ {pair['å­¦ç”Ÿ1']} å’Œ {pair['å­¦ç”Ÿ2']} çš„ä»£ç ç›¸ä¼¼åº¦é«˜è¾¾ {pair['ç›¸ä¼¼åº¦']:.1f}%")

            # æ˜¾ç¤ºé«˜ç›¸ä¼¼åº¦å­¦ç”Ÿå¯¹æ¯”
            st.subheader("é«˜ç›¸ä¼¼åº¦å­¦ç”Ÿå¯¹æ¯”")
            df = pd.DataFrame(report['high_similarity_pairs'])
            st.dataframe(df)

            # å¯è§†åŒ–å±•ç¤º
            st.subheader("ç›¸ä¼¼åº¦åˆ†å¸ƒ")
            similarities = [pair['ç›¸ä¼¼åº¦'] for pair in report['high_similarity_pairs']]
            # åˆ›å»ºæ•°æ®æ¡†

            sim_df = pd.DataFrame({'ç›¸ä¼¼åº¦': similarities})

            # åˆ›å»ºç›´æ–¹å›¾

            chart = alt.Chart(sim_df).mark_bar(color='salmon').encode(

                alt.X('ç›¸ä¼¼åº¦:Q', bin=alt.Bin(maxbins=10), title='ç›¸ä¼¼åº¦ (%)'),

                alt.Y('count()', title='é…å¯¹æ•°é‡'),

            ).properties(width=600, height=300)

            st.altair_chart(chart)
        else:
            st.success("âœ… æ²¡æœ‰å‘ç°é«˜ç›¸ä¼¼åº¦ä»£ç ")

    # æŠ„è¢­æ£€æµ‹å»ºè®®
    st.subheader("æŠ„è¢­æ£€æµ‹å»ºè®®")
    st.write("1. åŠ å¼ºä»£ç å®¡æŸ¥å’Œäººå·¥æ£€æŸ¥")
    st.write("2. ä½¿ç”¨æ›´å…ˆè¿›çš„æŠ„è¢­æ£€æµ‹å·¥å…·")
    st.write("3. å¯¹å­¦ç”Ÿè¿›è¡Œå­¦æœ¯è¯šä¿¡æ•™è‚²")
    st.write("4. è®¾è®¡æ›´å…·ä¸ªæ€§åŒ–çš„ç¼–ç¨‹é¢˜ç›®")
    st.write("5. å¢åŠ é¢è¯•ç¯èŠ‚éªŒè¯å­¦ç”Ÿç†è§£ç¨‹åº¦")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # åˆå§‹åŒ– session state
    init_session_state()

    # åˆ›å»ºä¾§è¾¹æ å¯¼èˆª
    st.sidebar.title("å¯¼èˆª")
    app_mode = st.sidebar.selectbox("é€‰æ‹©æ¨¡å¼", ["è¯„åˆ†ç•Œé¢", "åˆ›å»ºè¯„åˆ†é…ç½®", "åŠ è½½è¯„åˆ†é…ç½®", "å­¦æƒ…åé¦ˆ", "æŠ„è¢­æƒ…å†µ"])

    if app_mode == "åˆ›å»ºè¯„åˆ†é…ç½®":
        config = create_exam_config_ui()
        if config:
            st.session_state.exam_config = config
            st.success("è¯„åˆ†é…ç½®å·²åˆ›å»ºå¹¶åŠ è½½!")

    elif app_mode == "åŠ è½½è¯„åˆ†é…ç½®":
        config = load_exam_config_ui()
        if config:
            st.session_state.exam_config = config

    elif app_mode == "è¯„åˆ†ç•Œé¢":
        if st.session_state.exam_config:
            scoring_interface(st.session_state.exam_config)
        else:
            st.warning("è¯·å…ˆåˆ›å»ºæˆ–åŠ è½½è¯„åˆ†é…ç½®")

            # æä¾›é»˜è®¤é…ç½®
            if st.button("ä½¿ç”¨é»˜è®¤é…ç½®"):
                exam_config = {
                    "exam_name": "åµŒå…¥å¼ç³»ç»ŸæœŸä¸­è¯„åˆ†",
                    "exam_date": "2023-11-15",
                    "questions": [
                        {
                            "title": "LEDé—ªçƒæ§åˆ¶",
                            "description": "ç¼–å†™ç¨‹åºæ§åˆ¶å¼€å‘æ¿ä¸Šçš„LEDç¯ä»¥1Hzé¢‘ç‡é—ªçƒã€‚",
                            "total": 20,
                            "subtasks": [
                                {"desc": "æ­£ç¡®é…ç½®GPIOå¼•è„š", "score": 5},
                                {"desc": "å®ç°1ç§’å»¶æ—¶å‡½æ•°", "score": 5},
                                {"desc": "ä¸»å¾ªç¯ä¸­æ§åˆ¶LEDäº®ç­", "score": 10}
                            ],
                            "code_criteria": ["ä»£ç ç»“æ„æ¸…æ™°", "æ³¨é‡Šå®Œæ•´", "å˜é‡å‘½åè§„èŒƒ"]
                        },
                        {
                            "title": "ä¸²å£é€šä¿¡",
                            "description": "é…ç½®ä¸²å£ï¼Œå®ç°ä¸PCç«¯çš„å­—ç¬¦ä¸²æ”¶å‘ã€‚",
                            "total": 30,
                            "subtasks": [
                                {"desc": "æ­£ç¡®åˆå§‹åŒ–ä¸²å£", "score": 10},
                                {"desc": "æ¥æ”¶å¹¶å›æ˜¾å­—ç¬¦ä¸²", "score": 10},
                                {"desc": "å¤„ç†æ¥æ”¶ç¼“å†²åŒºæº¢å‡º", "score": 10}
                            ],
                            "code_criteria": ["ä»£ç å¥å£®æ€§", "èµ„æºç®¡ç†", "é”™è¯¯å¤„ç†"]
                        }
                    ]
                }
                st.session_state.exam_config = exam_config
                st.success("å·²åŠ è½½é»˜è®¤é…ç½®!")

    elif app_mode == "å­¦æƒ…åé¦ˆ":
        show_learning_feedback()

    elif app_mode == "æŠ„è¢­æƒ…å†µ":
        show_plagiarism_report()
